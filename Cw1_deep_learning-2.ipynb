{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpfbZnsqZxNx"
      },
      "source": [
        "# **Cw1 - Music Isolation and Genre Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrique Saldivar Corona"
      ],
      "metadata": {
        "id": "VHMUmY_xQMNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "ULOinWX8oqot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HaI2bwp8ZxNz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import torchvision.models as models\n",
        "import random\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import zipfile\n",
        "import warnings\n",
        "import traceback\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8uQeB5zHKwq",
        "outputId": "eba3042e-7fe7-4ae0-c21a-0c06e90958ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prioritize CUDA (NVIDIA GPU on Colab)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1vURgFlHSFN",
        "outputId": "06436e08-bb93-47aa-ad21-be9e4abb61d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1.1 copy to local storage"
      ],
      "metadata": {
        "id": "UBEG_juZQnzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Starting dataset copy (ZIP files) from Google Drive to Colab local storage.\")\n",
        "start_copy_time = time.time()\n",
        "\n",
        "# --- Define Source Paths on Google Drive ---\n",
        "\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/cw1_DL\" # Your project folder in Drive\n",
        "DRIVE_FMA_ZIP_PATH = os.path.join(DRIVE_PROJECT_PATH, \"fma_small.zip\") # Example name\n",
        "DRIVE_ESC50_ZIP_PATH = os.path.join(DRIVE_PROJECT_PATH, \"esc50_audio.zip\") # Example name (assuming you zipped only the 'audio' folder contents)\n",
        "DRIVE_FMA_METADATA_ZIP_PATH = os.path.join(DRIVE_PROJECT_PATH, \"fma_metadata.zip\") # Example name\n",
        "\n",
        "\n",
        "# --- Define Destination Paths on Colab Local Disk ---\n",
        "# Using /content/ is standard for temporary storage\n",
        "COLAB_TEMP_ZIP_PATH = \"/content/temp_zip\" # Temporary location to store zips before extraction\n",
        "COLAB_DATA_PATH = \"/content/data\"          # Final destination for extracted data\n",
        "\n",
        "# --- Define final extracted data paths (ensure these match Cell [27] Config) ---\n",
        "COLAB_FMA_AUDIO_PATH = os.path.join(COLAB_DATA_PATH, \"fma_small\")\n",
        "COLAB_ESC50_AUDIO_PATH = os.path.join(COLAB_DATA_PATH, \"esc50_audio\")\n",
        "COLAB_FMA_METADATA_PATH = os.path.join(COLAB_DATA_PATH, \"fma_metadata\")\n",
        "\n",
        "\n",
        "# --- Create Destination Folders ---\n",
        "# Clear previous data/zips if they exist\n",
        "if os.path.exists(COLAB_DATA_PATH):\n",
        "    print(f\"Removing existing local data directory: {COLAB_DATA_PATH}\")\n",
        "    shutil.rmtree(COLAB_DATA_PATH)\n",
        "if os.path.exists(COLAB_TEMP_ZIP_PATH):\n",
        "     print(f\"Removing existing temp zip directory: {COLAB_TEMP_ZIP_PATH}\")\n",
        "     shutil.rmtree(COLAB_TEMP_ZIP_PATH)\n",
        "\n",
        "print(f\"Creating local directories: {COLAB_TEMP_ZIP_PATH}, {COLAB_DATA_PATH}\")\n",
        "os.makedirs(COLAB_TEMP_ZIP_PATH, exist_ok=True)\n",
        "os.makedirs(COLAB_DATA_PATH, exist_ok=True)\n",
        "# Create subdirectories for extraction targets explicitly\n",
        "os.makedirs(COLAB_FMA_AUDIO_PATH, exist_ok=True)\n",
        "os.makedirs(COLAB_ESC50_AUDIO_PATH, exist_ok=True)\n",
        "os.makedirs(COLAB_FMA_METADATA_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Step 1: Copy ZIP files from Drive to Colab local temp storage ---\n",
        "copy_errors = False\n",
        "files_to_copy = {\n",
        "    \"FMA Audio ZIP\": DRIVE_FMA_ZIP_PATH,\n",
        "    \"ESC-50 Audio ZIP\": DRIVE_ESC50_ZIP_PATH,\n",
        "    \"FMA Metadata ZIP\": DRIVE_FMA_METADATA_ZIP_PATH,\n",
        "}\n",
        "\n",
        "copied_zip_paths = {}\n",
        "\n",
        "for name, drive_path in files_to_copy.items():\n",
        "    if os.path.exists(drive_path):\n",
        "        dest_path = os.path.join(COLAB_TEMP_ZIP_PATH, os.path.basename(drive_path))\n",
        "        print(f\"Copying {name} from {drive_path} to {dest_path}...\")\n",
        "        try:\n",
        "            shutil.copy(drive_path, dest_path)\n",
        "            copied_zip_paths[name] = dest_path # Store path for extraction\n",
        "            print(f\"{name} copy finished.\")\n",
        "        except Exception as e:\n",
        "            print(f\"!!! ERROR copying {name}: {e} !!!\")\n",
        "            copy_errors = True\n",
        "    else:\n",
        "        print(f\"!!! WARNING: Source ZIP file not found: {drive_path} !!!\")\n",
        "\n",
        "\n",
        "copy_end_time = time.time()\n",
        "print(f\"\\nZIP file copy attempt finished in {copy_end_time - start_copy_time:.2f} seconds.\")\n",
        "\n",
        "# --- Step 2: Extract ZIP files locally ---\n",
        "extract_start_time = time.time()\n",
        "print(\"\\nStarting extraction...\")\n",
        "\n",
        "if \"FMA Audio ZIP\" in copied_zip_paths:\n",
        "    print(f\"Extracting FMA audio ({copied_zip_paths['FMA Audio ZIP']}) to {COLAB_FMA_AUDIO_PATH}...\")\n",
        "    # Use -q for quiet, -d specifies destination. Overwrites existing files without prompt.\n",
        "    !unzip -q \"{copied_zip_paths['FMA Audio ZIP']}\" -d \"{COLAB_FMA_AUDIO_PATH}\"\n",
        "    print(\"FMA audio extraction finished.\")\n",
        "\n",
        "\n",
        "if \"ESC-50 Audio ZIP\" in copied_zip_paths:\n",
        "    print(f\"Extracting ESC-50 audio ({copied_zip_paths['ESC-50 Audio ZIP']}) to {COLAB_ESC50_AUDIO_PATH}...\")\n",
        "    !unzip -q \"{copied_zip_paths['ESC-50 Audio ZIP']}\" -d \"{COLAB_ESC50_AUDIO_PATH}\"\n",
        "    print(\"ESC-50 audio extraction finished.\")\n",
        "\n",
        "\n",
        "if \"FMA Metadata ZIP\" in copied_zip_paths:\n",
        "     print(f\"Extracting FMA metadata ({copied_zip_paths['FMA Metadata ZIP']}) to {COLAB_FMA_METADATA_PATH}...\")\n",
        "     !unzip -q \"{copied_zip_paths['FMA Metadata ZIP']}\" -d \"{COLAB_FMA_METADATA_PATH}\"\n",
        "     print(\"FMA metadata extraction finished.\")\n",
        "\n",
        "\n",
        "\n",
        "extract_end_time = time.time()\n",
        "print(f\"\\nExtraction finished in {extract_end_time - extract_start_time:.2f} seconds.\")\n",
        "total_time = time.time()\n",
        "print(f\"Total time (Copy + Extract): {total_time - start_copy_time:.2f} seconds.\")\n",
        "\n",
        "# --- Step 3: Verify extraction (Optional but recommended) ---\n",
        "if not copy_errors: # Only verify if copy seemed okay\n",
        "    print(\"\\nVerifying extracted files (listing counts/contents):\")\n",
        "    print(\"\\nFMA Audio subfolders (first level):\")\n",
        "    !ls \"{COLAB_FMA_AUDIO_PATH}\" | head -n 10 # List first 10 subfolders\n",
        "    !find \"{COLAB_FMA_AUDIO_PATH}\" -maxdepth 1 -type d | wc -l # Count total subfolders\n",
        "\n",
        "    print(\"\\nESC-50 Audio files:\")\n",
        "    !ls \"{COLAB_ESC50_AUDIO_PATH}\" | head -n 10 # List first 10 files\n",
        "    !ls \"{COLAB_ESC50_AUDIO_PATH}\" | wc -l # Count total files\n",
        "\n",
        "    print(\"\\nFMA Metadata files:\")\n",
        "    !ls \"{COLAB_FMA_METADATA_PATH}\" # List files in metadata\n",
        "else:\n",
        "    print(\"\\n!!! Verification skipped due to copy errors or missing files. Please check paths and ZIP contents. !!!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q68fn3caI0lK",
        "outputId": "6d4c35d5-312d-42ea-90cd-7ae83bf5348e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset copy (ZIP files) from Google Drive to Colab local storage...\n",
            "Removing existing local data directory: /content/data\n",
            "Removing existing temp zip directory: /content/temp_zip\n",
            "Creating local directories: /content/temp_zip, /content/data\n",
            "Copying FMA Audio ZIP from /content/drive/MyDrive/cw1_DL/fma_small.zip to /content/temp_zip/fma_small.zip...\n",
            "FMA Audio ZIP copy finished.\n",
            "Copying ESC-50 Audio ZIP from /content/drive/MyDrive/cw1_DL/esc50_audio.zip to /content/temp_zip/esc50_audio.zip...\n",
            "ESC-50 Audio ZIP copy finished.\n",
            "Copying FMA Metadata ZIP from /content/drive/MyDrive/cw1_DL/fma_metadata.zip to /content/temp_zip/fma_metadata.zip...\n",
            "FMA Metadata ZIP copy finished.\n",
            "\n",
            "ZIP file copy attempt finished in 33.07 seconds.\n",
            "\n",
            "Starting extraction...\n",
            "Extracting FMA audio (/content/temp_zip/fma_small.zip) to /content/data/fma_small...\n",
            "FMA audio extraction finished.\n",
            "Extracting ESC-50 audio (/content/temp_zip/esc50_audio.zip) to /content/data/esc50_audio...\n",
            "ESC-50 audio extraction finished.\n",
            "Extracting FMA metadata (/content/temp_zip/fma_metadata.zip) to /content/data/fma_metadata...\n",
            "FMA metadata extraction finished.\n",
            "\n",
            "Extraction finished in 92.30 seconds.\n",
            "Total time (Copy + Extract): 125.37 seconds.\n",
            "\n",
            "Verifying extracted files (listing counts/contents):\n",
            "\n",
            "FMA Audio subfolders (first level):\n",
            "fma_small\n",
            "__MACOSX\n",
            "3\n",
            "\n",
            "ESC-50 Audio files:\n",
            "audio\n",
            "__MACOSX\n",
            "2\n",
            "\n",
            "FMA Metadata files:\n",
            "fma_metadata  __MACOSX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD0WT0NbZxN1"
      },
      "source": [
        "# 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "\n",
        "# Data Paths (Ensure these point to LOCAL Colab data\n",
        "\n",
        "COLAB_DATA_PATH = \"/content/data\"\n",
        "# Check if local paths exist, otherwise fall back to Drive (optional, safer to rely on copy)\n",
        "if os.path.exists(os.path.join(COLAB_DATA_PATH, \"fma_small\", \"fma_small\")):\n",
        "    print(\"Using LOCAL Colab data paths (adjusting for nested folders)...\")\n",
        "    FMA_AUDIO_PATH = os.path.join(COLAB_DATA_PATH, \"fma_small\", \"fma_small\")\n",
        "    FMA_METADATA_PATH = os.path.join(COLAB_DATA_PATH, \"fma_metadata\", \"fma_metadata\")\n",
        "    AMBIENCE_DIR = os.path.join(COLAB_DATA_PATH, \"esc50_audio\", \"audio\")\n",
        "else:\n",
        "    print(\"WARNING: Local data not found, falling back to Drive paths (TRAINING WILL BE SLOW)\")\n",
        "    DRIVE_BASE_PATH = \"/content/drive/MyDrive/cw1_DL\"\n",
        "    FMA_AUDIO_PATH = os.path.join(DRIVE_BASE_PATH, \"fma_small\")\n",
        "    FMA_METADATA_PATH = os.path.join(DRIVE_BASE_PATH, \"fma_metadata\") # Check nesting\n",
        "    AMBIENCE_DIR = os.path.join(DRIVE_BASE_PATH, \"ESC-50-master\", \"audio\")\n",
        "\n",
        "# These variables now use the paths determined above\n",
        "MUSIC_DIR = FMA_AUDIO_PATH\n",
        "GENRE_DATA_DIR = FMA_AUDIO_PATH\n",
        "FMA_METADATA_CSV = os.path.join(FMA_METADATA_PATH, \"tracks.csv\")\n",
        "\n",
        "#  Output Directory Options\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/cw1_DL\"\n",
        "OUTPUT_DIR = os.path.join(DRIVE_PROJECT_PATH, \"output\")\n",
        "\n",
        "# Case Study File Path (Used after training)\n",
        "CASE_STUDY_FILE = os.path.join(DRIVE_PROJECT_PATH, \"Case_study_city.mp3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsYCRYrlpDaX",
        "outputId": "3fe1f01f-7465-4a6f-b990-0570cf5e696a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using LOCAL Colab data paths (adjusting for nested folders)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Parameters\n"
      ],
      "metadata": {
        "id": "PNYmRvaMX3xa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K7ym-XdZZxN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0b7dba-3532-4d9d-8cb2-8c658dec9d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Audio Chunk Duration: 5s\n",
            "Using Batch Size: 32\n",
            "Task 1 Epochs: 30, Task 2 Max Epochs: 30\n",
            "\n",
            "Configuration Set:\n",
            " - Output directory: /content/drive/MyDrive/cw1_DL/output\n",
            " - Attempting FMA audio from: /content/data/fma_small/fma_small\n",
            " - Attempting Ambience from: /content/data/esc50_audio/audio\n",
            " - Attempting Metadata from: /content/data/fma_metadata/fma_metadata/tracks.csv\n"
          ]
        }
      ],
      "source": [
        "# --- STFT Parameters ---\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "WIN_LENGTH = N_FFT\n",
        "WINDOW = 'hann'\n",
        "SAMPLE_RATE = 44100\n",
        "# --- CHANGE 1: Revert to 5-second chunks for Task 1 ---\n",
        "AUDIO_CHUNK_DURATION_S = 5\n",
        "AUDIO_CHUNK_SAMPLES = int(AUDIO_CHUNK_DURATION_S * SAMPLE_RATE)\n",
        "print(f\"Using Audio Chunk Duration: {AUDIO_CHUNK_DURATION_S}s\")\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "LEARNING_RATE_SEP = 1e-5 # Keep initial LR for scheduler\n",
        "LEARNING_RATE_CLS = 1e-3\n",
        "# --- CHANGE 2: Reduce Batch Size for 5s chunks ---\n",
        "BATCH_SIZE = 32 # Reduce from 64. If this *still* fails OOM, try 16 next.\n",
        "print(f\"Using Batch Size: {BATCH_SIZE}\")\n",
        "# --- CHANGE 3: Set Task 1 Epochs ---\n",
        "NUM_EPOCHS_SEP = 30 # Target for Task 1 (run fresh)\n",
        "NUM_EPOCHS_CLS = 30 # Max for Task 2 (Early stopping active)\n",
        "print(f\"Task 1 Epochs: {NUM_EPOCHS_SEP}, Task 2 Max Epochs: {NUM_EPOCHS_CLS}\")\n",
        "\n",
        "GENRE_CLASSES = ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock'] # Should be these 8\n",
        "NUM_GENRES = len(GENRE_CLASSES) # Should be 8\n",
        "GENRE_MAP = {name: i for i, name in enumerate(GENRE_CLASSES)} # This line MUST be present\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"\\nConfiguration Set:\")\n",
        "print(f\" - Output directory: {OUTPUT_DIR}\")\n",
        "print(f\" - Attempting FMA audio from: {MUSIC_DIR}\")\n",
        "print(f\" - Attempting Ambience from: {AMBIENCE_DIR}\")\n",
        "print(f\" - Attempting Metadata from: {FMA_METADATA_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mir_eval==0.7  # Install the mir_eval library, version 0.7 is usually suitable."
      ],
      "metadata": {
        "id": "BNZiWlfSGO8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade mir_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOJm202dXYLF",
        "outputId": "483eafb7-39ff-4a3b-c6ed-497f8b0e8f61"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mir_eval in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from mir_eval) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mir_eval) (1.15.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mir_eval) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSox67xOZxN2"
      },
      "source": [
        "3. Data handling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Data Handling"
      ],
      "metadata": {
        "id": "5WaiurStYUWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Handling\n",
        "\n",
        "\n",
        "#  Helper Functions\n",
        "\n",
        "def load_audio(path, target_sr=SAMPLE_RATE):\n",
        "    \"\"\"Loads audio file, converts to mono, and resamples.\"\"\"\n",
        "    waveform = None; sr = None\n",
        "    try:\n",
        "        waveform_ta, sr_ta = torchaudio.load(path); waveform = waveform_ta; sr = sr_ta\n",
        "    except Exception:\n",
        "        try:\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                waveform_lr, sr_lr = librosa.load(path, sr=target_sr, mono=True);\n",
        "            waveform = torch.from_numpy(waveform_lr).unsqueeze(0); sr = target_sr\n",
        "        except Exception: return None\n",
        "    if waveform is not None and waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "    if sr is not None and sr != target_sr and waveform is not None: waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n",
        "    if waveform is None: return None\n",
        "    return waveform.squeeze(0)\n",
        "\n",
        "def create_mixture(music_wav, ambience_wav, target_snr_db):\n",
        "    \"\"\"Creates a mixture of music and ambience at a target SNR.\"\"\"\n",
        "    if not isinstance(music_wav, torch.Tensor): music_wav = torch.tensor(music_wav);\n",
        "    if not isinstance(ambience_wav, torch.Tensor): ambience_wav = torch.tensor(ambience_wav);\n",
        "    len_music = music_wav.shape[0]; len_ambience = ambience_wav.shape[0]\n",
        "    if len_ambience < len_music: ambience_wav = ambience_wav.repeat(len_music // len_ambience + 1)[:len_music]\n",
        "    elif len_ambience > len_music: start = np.random.randint(0, len_ambience - len_music + 1); ambience_wav = ambience_wav[start : start + len_music]\n",
        "    power_music = torch.mean(music_wav**2); power_ambience = torch.mean(ambience_wav**2)\n",
        "    if power_music < 1e-10 or power_ambience < 1e-10: return music_wav, music_wav\n",
        "    snr_linear = 10**(target_snr_db / 10.0); target_power_ambience = power_music / snr_linear\n",
        "    scaling_factor = torch.sqrt(target_power_ambience / (power_ambience + 1e-8)); scaled_ambience_wav = ambience_wav * scaling_factor\n",
        "    mixture_wav = music_wav + scaled_ambience_wav; max_amp = torch.max(torch.abs(mixture_wav))\n",
        "    if max_amp > 1.0: mixture_wav /= max_amp\n",
        "    return mixture_wav, music_wav\n",
        "\n",
        "def get_spectrogram(waveform, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, window=WINDOW):\n",
        "    \"\"\"Computes the STFT (magnitude and phase) of a waveform ON CPU.\"\"\"\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        window_fn = torch.hann_window; stft_transform = torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length, window_fn=window_fn, power=None, return_complex=True)\n",
        "\n",
        "        spectrogram_complex = stft_transform(waveform.cpu());\n",
        "    magnitude = torch.abs(spectrogram_complex); phase = torch.angle(spectrogram_complex); return magnitude, phase\n",
        "\n",
        "def waveform_from_spectrogram(magnitude, phase, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, window=WINDOW):\n",
        "    \"\"\"Reconstructs waveform from magnitude and phase using iSTFT.\"\"\"\n",
        "\n",
        "    magnitude = magnitude.to(device); phase = phase.to(device); stft_complex = torch.polar(magnitude, phase); window_fn = torch.hann_window\n",
        "    istft_transform = torchaudio.transforms.InverseSpectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length, window_fn=window_fn).to(device)\n",
        "    est_len = int(phase.shape[-1] * hop_length); waveform = istft_transform(stft_complex, length=est_len); return waveform\n",
        "\n",
        "\n",
        "# Dataset Classes\n",
        "\n",
        "\n",
        "class MusicAmbienceDataset(Dataset):\n",
        "    def __init__(self, music_files, ambience_files, target_snr_db_range=(-5, 10), chunk_samples=AUDIO_CHUNK_SAMPLES, sr=SAMPLE_RATE):\n",
        "        self.music_files = music_files; self.ambience_files = ambience_files; self.target_snr_db_range = target_snr_db_range;\n",
        "        self.chunk_samples = chunk_samples; self.sr = sr;\n",
        "        if not music_files or not ambience_files: raise ValueError(\"Music or ambience file list is empty!\")\n",
        "        print(f\"Initialized MusicAmbienceDataset with {len(music_files)} music files.\")\n",
        "\n",
        "    def __len__(self): return len(self.music_files) * 3\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        music_path = None\n",
        "        ambience_path = None\n",
        "        try:\n",
        "            # Determine file paths BEFORE the main try block for better error reporting\n",
        "            music_idx = idx % len(self.music_files)\n",
        "            ambience_idx = random.randint(0, len(self.ambience_files) - 1)\n",
        "            music_path = self.music_files[music_idx]\n",
        "            ambience_path = self.ambience_files[ambience_idx]\n",
        "\n",
        "            # Main processing\n",
        "            music_wav = load_audio(music_path, target_sr=self.sr)\n",
        "\n",
        "            if music_wav is None or len(music_wav) < self.chunk_samples:\n",
        "\n",
        "                 return None\n",
        "\n",
        "            ambience_wav = load_audio(ambience_path, target_sr=self.sr)\n",
        "            if ambience_wav is None:\n",
        "\n",
        "                return None\n",
        "\n",
        "            #  Chunking and Mixing\n",
        "            start_idx = random.randint(0, len(music_wav) - self.chunk_samples)\n",
        "            music_chunk = music_wav[start_idx : start_idx + self.chunk_samples]\n",
        "            target_snr = random.uniform(self.target_snr_db_range[0], self.target_snr_db_range[1])\n",
        "            mixture_chunk, clean_music_chunk = create_mixture(music_chunk, ambience_wav, target_snr)\n",
        "\n",
        "            #  Spectrograms\n",
        "            mixture_mag, mixture_phase = get_spectrogram(mixture_chunk)\n",
        "            clean_music_mag, _ = get_spectrogram(clean_music_chunk) # Uses CPU get_spectrogram\n",
        "\n",
        "            # Mask Calculation\n",
        "            mask = clean_music_mag / (mixture_mag + 1e-8)\n",
        "            mask = torch.clamp(mask, 0.0, 1.0)\n",
        "\n",
        "\n",
        "            return mixture_mag.cpu(), mask.cpu(), mixture_phase.cpu(), clean_music_chunk.cpu()\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "            print(f\"!!! ERROR in MusicAmbienceDataset __getitem__ (Worker Crashed?) !!!\")\n",
        "            print(f\"!!! Index: {idx}\")\n",
        "            print(f\"!!! Music Path: {music_path}\")\n",
        "            print(f\"!!! Ambience Path: {ambience_path}\")\n",
        "            print(f\"!!! Error Type: {type(e).__name__}\")\n",
        "            print(f\"!!! Error Message: {e}\")\n",
        "\n",
        "            traceback.print_exc()\n",
        "            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "class GenreDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, chunk_samples=AUDIO_CHUNK_SAMPLES, sr=SAMPLE_RATE, n_mels=128):\n",
        "        self.file_paths = file_paths; self.labels = labels; self.chunk_samples = chunk_samples; self.sr = sr; self.n_mels = n_mels\n",
        "\n",
        "        self.mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.sr, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, n_mels=self.n_mels)\n",
        "        self.log_mel_spec_transform = torchaudio.transforms.AmplitudeToDB()\n",
        "        print(f\"Initialized GenreDataset with {len(file_paths)} files.\")\n",
        "\n",
        "    def __len__(self): return len(self.file_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            path = self.file_paths[idx]; label = self.labels[idx]; waveform = load_audio(path, target_sr=self.sr)\n",
        "            if waveform is None or len(waveform) < self.chunk_samples: return None\n",
        "            start_idx = random.randint(0, len(waveform) - self.chunk_samples); chunk = waveform[start_idx : start_idx + self.chunk_samples]\n",
        "\n",
        "            mel_spec = self.mel_spectrogram_transform(chunk.cpu()); log_mel_spec = self.log_mel_spec_transform(mel_spec)\n",
        "            return log_mel_spec, torch.tensor(label, dtype=torch.long)\n",
        "        except Exception as e:\n",
        "\n",
        "            return None\n",
        "\n",
        "#  Define collate_fn\n",
        "def collate_skip_none(batch):\n",
        "    \"\"\"Custom collate function that filters out None items from a batch.\"\"\"\n",
        "    filtered_batch = [item for item in batch if item is not None]\n",
        "    if not filtered_batch: return None\n",
        "    try: return torch.utils.data.default_collate(filtered_batch)\n",
        "    except Exception as e:\n",
        "\n",
        "        return None\n",
        "\n",
        "# Data Preparation Logic\n",
        "\n",
        "print(\"--- Data Preparation ---\")\n",
        "\n",
        "#  Step 1: Find initial lists of files\n",
        "print(\"Scanning for initial file lists from local Colab storage...\")\n",
        "\n",
        "initial_music_files = glob.glob(os.path.join(MUSIC_DIR, '**', '*.mp3'), recursive=True)\n",
        "all_ambience_files = glob.glob(os.path.join(AMBIENCE_DIR, '*.wav'), recursive=False)\n",
        "print(f\"Found {len(initial_music_files)} initial music files (Local).\")\n",
        "print(f\"Found {len(all_ambience_files)} ambience files (Local).\")\n",
        "if not initial_music_files: print(f\"!!! WARNING: Initial music file list empty. Check MUSIC_DIR path: {MUSIC_DIR}\")\n",
        "if not all_ambience_files: print(f\"!!! WARNING: Ambience file list empty. Check AMBIENCE_DIR path: {AMBIENCE_DIR}\")\n",
        "\n",
        "# Step 2: Pre-filter problematic FMA files\n",
        "\n",
        "print(\"\\nPre-filtering FMA music files from local storage...\")\n",
        "valid_music_files = []\n",
        "invalid_music_files_count = 0\n",
        "start_filter_time = time.time()\n",
        "for i, fpath in enumerate(initial_music_files):\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"  Checked {i+1}/{len(initial_music_files)} files...\")\n",
        "    try:\n",
        "        wav = load_audio(fpath)\n",
        "        if wav is not None and len(wav) > AUDIO_CHUNK_SAMPLES:\n",
        "            valid_music_files.append(fpath)\n",
        "        else:\n",
        "            invalid_music_files_count += 1\n",
        "    except Exception as e_filter:\n",
        "        invalid_music_files_count += 1\n",
        "end_filter_time = time.time()\n",
        "print(f\"\\nPre-filtering complete in {end_filter_time - start_filter_time:.2f} seconds.\")\n",
        "print(f\"Found {len(valid_music_files)} valid music files.\")\n",
        "if invalid_music_files_count > 0:\n",
        "    print(f\"Excluded {invalid_music_files_count} problematic or short music files during pre-filtering.\")\n",
        "\n",
        "all_music_files = valid_music_files\n",
        "random.shuffle(all_music_files)\n",
        "\n",
        "# Step 3: Map Valid FMA Files to Genres\n",
        "\n",
        "print(\"\\nMapping valid FMA files to genres...\")\n",
        "all_genre_files = []\n",
        "all_genre_labels = []\n",
        "try:\n",
        "    # Uses FMA_METADATA_CSV which points to nested local copy\n",
        "    if not os.path.exists(FMA_METADATA_CSV): raise FileNotFoundError(f\"Metadata CSV not found at {FMA_METADATA_CSV}\")\n",
        "    tracks_df = pd.read_csv(FMA_METADATA_CSV, index_col=0, header=[0, 1])\n",
        "    small_subset_indices = tracks_df[tracks_df[('set', 'subset')] == 'small'].index\n",
        "    print(f\"Checking {len(small_subset_indices)} tracks from FMA small subset against valid files...\")\n",
        "    valid_music_files_dict = {os.path.basename(f): f for f in all_music_files}\n",
        "    processed_count = 0\n",
        "    for track_id in small_subset_indices:\n",
        "        tid_str = f\"{track_id:06d}\"\n",
        "        filename = f\"{tid_str}.mp3\"\n",
        "        if filename in valid_music_files_dict:\n",
        "            full_path = valid_music_files_dict[filename]\n",
        "            track_info = tracks_df.loc[track_id]\n",
        "            genre_name = track_info[('track', 'genre_top')]\n",
        "            if genre_name in GENRE_MAP:\n",
        "                all_genre_files.append(full_path)\n",
        "                all_genre_labels.append(GENRE_MAP[genre_name])\n",
        "                processed_count += 1\n",
        "    print(f\"Mapped {processed_count} valid small subset tracks to genres.\")\n",
        "except FileNotFoundError as e: print(f\"!!! ERROR: {e}\")\n",
        "except ImportError: print(\"!!! ERROR: pandas library is required. `pip install pandas`\")\n",
        "except KeyError as e: print(f\"!!! ERROR: Column name issue reading FMA metadata CSV (Maybe {e} ?).\")\n",
        "except Exception as e: print(f\"Error during FMA genre mapping: {e}\")\n",
        "\n",
        "print(f\"Total genre files prepared for Task 2: {len(all_genre_files)}\")\n",
        "if not all_genre_files: print(\"!!! WARNING: Genre file list empty after filtering/mapping.\")\n",
        "\n",
        "\n",
        "#  Step 4: Data Splitting\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "music_train_files, music_val_files, music_test_files = [], [], []\n",
        "genre_files_train, genre_files_val, genre_files_test = [], [], []\n",
        "genre_labels_train, genre_labels_val, genre_labels_test = [], [], []\n",
        "\n",
        "if all_music_files:\n",
        "    music_train_val, music_test_files = train_test_split(all_music_files, test_size=0.15, random_state=42)\n",
        "    if len(music_train_val) > 1:\n",
        "        music_train_files, music_val_files = train_test_split(music_train_val, test_size=0.1765, random_state=42)\n",
        "    else: music_train_files = music_train_val\n",
        "\n",
        "if all_genre_files and len(all_genre_files) == len(all_genre_labels):\n",
        "    unique_labels, counts = np.unique(all_genre_labels, return_counts=True)\n",
        "    min_samples_for_stratify = 3\n",
        "    can_stratify = counts.min() >= min_samples_for_stratify if len(counts) > 0 else False\n",
        "    stratify_opt = all_genre_labels if can_stratify else None\n",
        "    if not can_stratify: print(\"Warning: Cannot stratify genre split (min samples per class < 3).\")\n",
        "\n",
        "    genre_files_train_val, genre_files_test, genre_labels_train_val, genre_labels_test = train_test_split(\n",
        "        all_genre_files, all_genre_labels, test_size=0.15, stratify=stratify_opt, random_state=42)\n",
        "    stratify_opt_2 = genre_labels_train_val if can_stratify else None\n",
        "    if len(genre_files_train_val) > 1:\n",
        "         genre_files_train, genre_files_val, genre_labels_train, genre_labels_val = train_test_split(\n",
        "             genre_files_train_val, genre_labels_train_val, test_size=0.1765, stratify=stratify_opt_2, random_state=42)\n",
        "    else: genre_files_train, genre_labels_train = genre_files_train_val, genre_labels_train_val\n",
        "else:\n",
        "    print(\"Warning: Cannot split genre data.\")\n",
        "\n",
        "print(f\"Task 1 Splits: Train={len(music_train_files)}, Val={len(music_val_files)}, Test={len(music_test_files)}\")\n",
        "print(f\"Task 2 Splits: Train={len(genre_files_train)}, Val={len(genre_files_val)}, Test={len(genre_files_test)}\")\n",
        "\n",
        "\n",
        "#  Step 5: Create Datasets and DataLoaders\n",
        "print(\"\\nCreating Datasets and DataLoaders...\")\n",
        "\n",
        "num_workers_flag = 2\n",
        "\n",
        "pin_memory_flag = True if device.type == 'cuda' else False\n",
        "\n",
        "train_sep_dataset = MusicAmbienceDataset(music_train_files, all_ambience_files) if music_train_files and all_ambience_files else None\n",
        "val_sep_dataset = MusicAmbienceDataset(music_val_files, all_ambience_files) if music_val_files and all_ambience_files else None\n",
        "test_sep_dataset = MusicAmbienceDataset(music_test_files, all_ambience_files) if music_test_files and all_ambience_files else None\n",
        "\n",
        "train_genre_dataset = GenreDataset(genre_files_train, genre_labels_train) if genre_files_train else None\n",
        "val_genre_dataset = GenreDataset(genre_files_val, genre_labels_val) if genre_files_val else None\n",
        "test_genre_dataset = GenreDataset(genre_files_test, genre_labels_test) if genre_files_test else None\n",
        "\n",
        "\n",
        "train_sep_loader = DataLoader(train_sep_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers_flag, collate_fn=collate_skip_none, pin_memory=pin_memory_flag, persistent_workers=True if num_workers_flag > 0 else False) if train_sep_dataset else None\n",
        "val_sep_loader = DataLoader(val_sep_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_flag, collate_fn=collate_skip_none, pin_memory=pin_memory_flag, persistent_workers=True if num_workers_flag > 0 else False) if val_sep_dataset else None\n",
        "test_sep_loader = DataLoader(test_sep_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_flag, collate_fn=collate_skip_none, pin_memory=pin_memory_flag, persistent_workers=True if num_workers_flag > 0 else False) if test_sep_dataset else None\n",
        "\n",
        "train_genre_loader = DataLoader(train_genre_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers_flag, collate_fn=collate_skip_none, pin_memory=pin_memory_flag, persistent_workers=True if num_workers_flag > 0 else False) if train_genre_dataset else None\n",
        "val_genre_loader = DataLoader(val_genre_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_flag, collate_fn=collate_skip_none, pin_memory=pin_memory_flag, persistent_workers=True if num_workers_flag > 0 else False) if val_genre_dataset else None\n",
        "test_genre_loader = DataLoader(test_genre_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_flag, collate_fn=collate_skip_none, pin_memory=pin_memory_flag, persistent_workers=True if num_workers_flag > 0 else False) if test_genre_dataset else None\n",
        "\n",
        "print(f\"Train Sep Loader: {'Created' if train_sep_loader else 'Not Created'} (Workers: {num_workers_flag})\")\n",
        "print(f\"Val Sep Loader: {'Created' if val_sep_loader else 'Not Created'} (Workers: {num_workers_flag})\")\n",
        "print(f\"Test Sep Loader: {'Created' if test_sep_loader else 'Not Created'} (Workers: {num_workers_flag})\")\n",
        "print(f\"Train Genre Loader: {'Created' if train_genre_loader else 'Not Created'} (Workers: {num_workers_flag})\")\n",
        "print(f\"Val Genre Loader: {'Created' if val_genre_loader else 'Not Created'} (Workers: {num_workers_flag})\")\n",
        "print(f\"Test Genre Loader: {'Created' if test_genre_loader else 'Not Created'} (Workers: {num_workers_flag})\")\n",
        "print(\"Data preparation section finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twk3ofatehzz",
        "outputId": "d5ae90c2-60ff-4f7e-f2f6-ac82d1c3ab40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Preparation ---\n",
            "Scanning for initial file lists from local Colab storage...\n",
            "Found 8000 initial music files (Local).\n",
            "Found 2000 ambience files (Local).\n",
            "\n",
            "Pre-filtering FMA music files from local storage...\n",
            "  Checked 1000/8000 files...\n",
            "  Checked 2000/8000 files...\n",
            "  Checked 3000/8000 files...\n",
            "  Checked 4000/8000 files...\n",
            "  Checked 5000/8000 files...\n",
            "  Checked 6000/8000 files...\n",
            "  Checked 7000/8000 files...\n",
            "  Checked 8000/8000 files...\n",
            "\n",
            "Pre-filtering complete in 528.87 seconds.\n",
            "Found 7994 valid music files.\n",
            "Excluded 6 problematic or short music files during pre-filtering.\n",
            "\n",
            "Mapping valid FMA files to genres...\n",
            "Checking 8000 tracks from FMA small subset against valid files...\n",
            "Mapped 7994 valid small subset tracks to genres.\n",
            "Total genre files prepared for Task 2: 7994\n",
            "\n",
            "Splitting data...\n",
            "Task 1 Splits: Train=5594, Val=1200, Test=1200\n",
            "Task 2 Splits: Train=5594, Val=1200, Test=1200\n",
            "\n",
            "Creating Datasets and DataLoaders...\n",
            "Initialized MusicAmbienceDataset with 5594 music files.\n",
            "Initialized MusicAmbienceDataset with 1200 music files.\n",
            "Initialized MusicAmbienceDataset with 1200 music files.\n",
            "Initialized GenreDataset with 5594 files.\n",
            "Initialized GenreDataset with 1200 files.\n",
            "Initialized GenreDataset with 1200 files.\n",
            "Train Sep Loader: Created (Workers: 2)\n",
            "Val Sep Loader: Created (Workers: 2)\n",
            "Test Sep Loader: Created (Workers: 2)\n",
            "Train Genre Loader: Created (Workers: 2)\n",
            "Val Genre Loader: Created (Workers: 2)\n",
            "Test Genre Loader: Created (Workers: 2)\n",
            "Data preparation section finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsxIHe9yZxN2"
      },
      "source": [
        " # 4. Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the deep learning models for separation (Task 1) and classification (Task 2).\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models # For pre-trained models\n",
        "\n",
        "# Task 1: Separation (U-Net)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"(Conv2d => BN => LeakyReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"ConvBlock followed by MaxPool\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_block(x)\n",
        "        pooled = self.pool(skip)\n",
        "        return skip, pooled\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"UpConv + Concatenate + ConvBlock + Optional Dropout\"\"\"\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        self.conv_block = ConvBlock(out_channels + skip_channels, out_channels)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(p=dropout_p) if dropout_p > 0 else nn.Identity() # Identity if no dropout\n",
        "\n",
        "    def _match_height_width(self, up_x, skip_x):\n",
        "        \"\"\"Helper to crop skip connection if needed after up-convolution.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        h_diff = skip_x.shape[2] - up_x.shape[2]\n",
        "        w_diff = skip_x.shape[3] - up_x.shape[3]\n",
        "        if h_diff > 0 or w_diff > 0:\n",
        "\n",
        "\n",
        "             crop_h_start = h_diff // 2\n",
        "             crop_w_start = w_diff // 2\n",
        "             skip_x = skip_x[:, :, crop_h_start:crop_h_start + up_x.shape[2], crop_w_start:crop_w_start + up_x.shape[3]]\n",
        "        elif h_diff < 0 or w_diff < 0:\n",
        "\n",
        "             up_x = nn.functional.pad(up_x, (-w_diff // 2, w_diff - (-w_diff // 2),\n",
        "                                            -h_diff // 2, h_diff - (-h_diff // 2)))\n",
        "\n",
        "        return up_x, skip_x\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "\n",
        "\n",
        "        up_x = self.upconv(x)\n",
        "        # Match spatial dimensions before concatenation\n",
        "        up_x_matched, skip_matched = self._match_height_width(up_x, skip)\n",
        "        # Concatenate along the channel dimension\n",
        "        concat_x = torch.cat([up_x_matched, skip_matched], dim=1)\n",
        "        # Apply dropout\n",
        "        concat_x = self.dropout(concat_x)\n",
        "        # Apply convolutional block\n",
        "        out = self.conv_block(concat_x)\n",
        "        return out\n",
        "\n",
        "class SeparationUNet(nn.Module):\n",
        "    \"\"\"More standard U-Net for audio source separation (mask estimation).\"\"\"\n",
        "    def __init__(self, n_channels=1, n_classes=1, features=[16, 32, 64, 128, 256]):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "\n",
        "        # Encoder Path\n",
        "        in_ch = n_channels\n",
        "        for feature in features:\n",
        "            self.encoders.append(EncoderBlock(in_ch, feature))\n",
        "            in_ch = feature\n",
        "        print(f\"U-Net Encoder features: {features}\")\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck_features = features[-1] * 2\n",
        "        self.bottleneck = ConvBlock(features[-1], bottleneck_features)\n",
        "        print(f\"U-Net Bottleneck features: {bottleneck_features}\")\n",
        "\n",
        "        # Decoder Path\n",
        "\n",
        "        decoder_features = features[::-1]\n",
        "        in_ch_decode = bottleneck_features\n",
        "        for i in range(len(decoder_features)):\n",
        "            skip_ch = decoder_features[i]\n",
        "            out_ch = decoder_features[i]\n",
        "            self.decoders.append(DecoderBlock(in_ch_decode, skip_ch, out_ch))\n",
        "            in_ch_decode = out_ch\n",
        "        print(f\"U-Net Decoder features (output channels per block): {decoder_features}\")\n",
        "\n",
        "        # Final Output Convolution\n",
        "\n",
        "        self.final_conv = nn.Conv2d(features[0], n_classes, kernel_size=1)\n",
        "        # Sigmoid activation to ensure mask output is between 0 and 1\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        #  Encoder\n",
        "        for encoder in self.encoders:\n",
        "            skip, x = encoder(x)\n",
        "            skip_connections.append(skip)\n",
        "\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "\n",
        "        #  Decoder\n",
        "\n",
        "        skip_connections = skip_connections[::-1] # Reverse list\n",
        "        for i in range(len(self.decoders)):\n",
        "            skip = skip_connections[i]\n",
        "            x = self.decoders[i](x, skip)\n",
        "\n",
        "\n",
        "        # --- Final Output ---\n",
        "        mask = self.final_conv(x)\n",
        "        mask = self.final_activation(mask)\n",
        "\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n",
        "# Task 2: Classification Model\n",
        "# Using a pre-trained ResNet18 adapted for audio\n",
        "\n",
        "def get_pretrained_genre_classifier(num_genres=NUM_GENRES, pretrained=True):\n",
        "    \"\"\"Loads a pre-trained ResNet18 model and adapts it for genre classification.\"\"\"\n",
        "    print(f\"Loading {'pre-trained' if pretrained else 'randomly initialized'} ResNet18 model...\")\n",
        "    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "    model = models.resnet18(weights=weights)\n",
        "\n",
        "    # Adapt Input Layer\n",
        "    original_conv1 = model.conv1\n",
        "    model.conv1 = nn.Conv2d(1, original_conv1.out_channels, kernel_size=original_conv1.kernel_size,\n",
        "                            stride=original_conv1.stride, padding=original_conv1.padding, bias=False)\n",
        "    print(\"Adapted model.conv1 to accept 1 input channel.\")\n",
        "    if pretrained:\n",
        "        try:\n",
        "             original_weights_tensor = original_conv1.weight.data\n",
        "        except AttributeError:\n",
        "             original_weights_tensor = weights.state_dict()['conv1.weight']\n",
        "        model.conv1.weight.data = torch.mean(original_weights_tensor, dim=1, keepdim=True)\n",
        "        print(\"Initialized new conv1 weights by averaging original RGB weights.\")\n",
        "\n",
        "    # Adapt Output Layer\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_genres)\n",
        "    print(f\"Adapted model.fc to output {num_genres} classes.\")\n",
        "    return model\n",
        "\n",
        "print(\"Model definitions updated: Using enhanced U-Net for Task 1.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh2tEY3aXShl",
        "outputId": "3e3288b9-7efd-4dad-aabf-39694f9c75dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model definitions updated: Using enhanced U-Net for Task 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB-8C11RHeKt"
      },
      "source": [
        "# 5. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop Definitions\n",
        "# Includes interpolation for shape matching\n",
        "\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import torch\n",
        "import torch.nn.functional as F # For interpolate\n",
        "from sklearn.metrics import accuracy_score, f1_score # Evaluation metrics\n",
        "\n",
        "# --- Function: train_epoch ---\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device, task_type='separation', epoch_num=0, total_epochs=1):\n",
        "    \"\"\"Runs a single training epoch with a tqdm progress bar.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_items = 0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch_num}/{total_epochs} [{task_type.capitalize()} Train]\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        if batch is None: continue\n",
        "\n",
        "        if task_type == 'separation':\n",
        "             if len(batch) < 2: continue\n",
        "             inputs, targets = batch[0], batch[1] # mixture_mag, mask\n",
        "        elif task_type == 'classification':\n",
        "             if len(batch) < 2: continue\n",
        "             inputs, targets = batch[0], batch[1] # log_mel_spec, label\n",
        "        else: raise ValueError(\"Invalid task_type for training\")\n",
        "\n",
        "        if inputs is None or targets is None or inputs.nelement() == 0 or targets.nelement() == 0: continue\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        if inputs.dim() == 3: inputs = inputs.unsqueeze(1)\n",
        "        if task_type == 'separation' and targets.dim() == 3: targets = targets.unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Interpolate output shape to match target\n",
        "        if task_type == 'separation' and outputs.shape != targets.shape:\n",
        "            # Get target spatial dimensions (Height = Freq, Width = Time)\n",
        "            target_h, target_w = targets.shape[2], targets.shape[3]\n",
        "            # Resize output to match target size using bilinear interpolation\n",
        "            outputs = F.interpolate(outputs, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "            if batch_idx == 0 and epoch_num == 1: # Print message only once\n",
        "                 print(f\"Interpolated output shape to match target: {outputs.shape}\")\n",
        "        # End interpolation\n",
        "\n",
        "        # Now calculate loss - shapes should match for separation task\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        if batch_idx == 0 and epoch_num == 1: # Print loss for first batch\n",
        "            print(f\"Loss value (first batch): {loss.item():.6f}\")\n",
        "            # Note: Removed the larger debug block here\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"Warning: NaN loss detected at batch {batch_idx}. Skipping batch.\")\n",
        "            continue\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        batch_size = inputs.size(0)\n",
        "        total_loss += batch_loss * batch_size\n",
        "        num_items += batch_size\n",
        "\n",
        "        if num_items > 0:\n",
        "            progress_bar.set_postfix(loss=f\"{(total_loss / num_items):.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_items if num_items > 0 else 0\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# --- Function: validate_epoch ---\n",
        "def validate_epoch(model, dataloader, criterion, device, task_type='separation', epoch_num=0, total_epochs=1):\n",
        "    \"\"\"Runs a single validation epoch with a tqdm progress bar.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_items = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch_num}/{total_epochs} [{task_type.capitalize()} Val]\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            if batch is None: continue\n",
        "\n",
        "            if task_type == 'separation':\n",
        "                 if len(batch) < 2: continue\n",
        "                 inputs, targets = batch[0], batch[1] # mixture_mag, mask\n",
        "            elif task_type == 'classification':\n",
        "                 if len(batch) < 2: continue\n",
        "                 inputs, targets = batch[0], batch[1] # log_mel_spec, label\n",
        "            else: raise ValueError(\"Invalid task_type for validation\")\n",
        "\n",
        "            if inputs is None or targets is None or inputs.nelement() == 0 or targets.nelement() == 0: continue\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            if inputs.dim() == 3: inputs = inputs.unsqueeze(1)\n",
        "            if task_type == 'separation' and targets.dim() == 3: targets = targets.unsqueeze(1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Interpolate output shape to match target\n",
        "            if task_type == 'separation' and outputs.shape != targets.shape:\n",
        "                target_h, target_w = targets.shape[2], targets.shape[3]\n",
        "                outputs = F.interpolate(outputs, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "            # End interpolation\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"Warning: NaN validation loss detected at batch {batch_idx}. Skipping batch.\")\n",
        "                continue\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            num_items += inputs.size(0)\n",
        "\n",
        "            if task_type == 'classification':\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            if num_items > 0:\n",
        "                 progress_bar.set_postfix(loss=f\"{(total_loss / num_items):.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_items if num_items > 0 else 0\n",
        "    val_metrics = {'loss': avg_loss}\n",
        "\n",
        "    if task_type == 'classification' and all_targets:\n",
        "        accuracy = accuracy_score(all_targets, all_preds)\n",
        "        f1 = f1_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
        "        val_metrics['accuracy'] = accuracy\n",
        "        val_metrics['f1'] = f1\n",
        "\n",
        "    return val_metrics"
      ],
      "metadata": {
        "id": "hNNVBmIQYf-w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z1XNLEIHcFk"
      },
      "source": [
        "# 6. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation functions\n",
        "\n",
        "\n",
        "try:\n",
        "    import mir_eval\n",
        "    MIR_EVAL_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"WARNING: mir_eval not installed. SDR calculation will be skipped.\")\n",
        "    MIR_EVAL_AVAILABLE = False\n",
        "\n",
        "# function: calculate_sdr\n",
        "def calculate_sdr(estimated_source_wav, true_source_wav):\n",
        "    \"\"\"Calculates SDR using mir_eval (if available). Attempts float64 casting.\"\"\"\n",
        "    if not MIR_EVAL_AVAILABLE:\n",
        "        return 0.0 # return 0 if mir_eval not installed\n",
        "    try:\n",
        "        # ensure inputs are numpy arrays on cpu\n",
        "        if not isinstance(estimated_source_wav, np.ndarray): estimated_source_wav = estimated_source_wav.cpu().numpy()\n",
        "        if not isinstance(true_source_wav, np.ndarray): true_source_wav = true_source_wav.cpu().numpy()\n",
        "\n",
        "        # ensure inputs are 1d\n",
        "        if estimated_source_wav.ndim > 1: estimated_source_wav = estimated_source_wav.squeeze()\n",
        "        if true_source_wav.ndim > 1: true_source_wav = true_source_wav.squeeze()\n",
        "        if estimated_source_wav.ndim != 1 or true_source_wav.ndim != 1:\n",
        "             return 0.0 # return 0 if dimensions are wrong\n",
        "\n",
        "        min_len = min(len(estimated_source_wav), len(true_source_wav))\n",
        "        if min_len == 0: return 0.0 # avoid empty arrays\n",
        "\n",
        "        # cast to float64 and add epsilon\n",
        "        est = estimated_source_wav[:min_len].astype(np.float64)\n",
        "        true = true_source_wav[:min_len].astype(np.float64)\n",
        "        # add tiny noise to prevent potential perfect silence issues in calculation\n",
        "        epsilon = 1e-10\n",
        "        est += np.random.randn(est.shape[0]) * epsilon\n",
        "        true += np.random.randn(true.shape[0]) * epsilon\n",
        "        # end modification\n",
        "\n",
        "        # add check for silence or near-silence in ground truth after adding epsilon\n",
        "        if np.sum(np.abs(true)) < 1e-8:\n",
        "            return 0.0\n",
        "\n",
        "        # mir_eval expects shape (n_sources, n_samples)\n",
        "        sdr_value, _, _, _ = mir_eval.separation.bss_eval_sources(true[np.newaxis, :], est[np.newaxis, :])\n",
        "\n",
        "        # return 0 if mir_eval calculation itself fails in a way that produces nan\n",
        "        if np.isnan(sdr_value[0]):\n",
        "             return 0.0 # or return np.nan if you want to handle it differently later\n",
        "\n",
        "        return sdr_value[0]\n",
        "\n",
        "    except ValueError as ve:\n",
        "        # error during sdr calculation (valueerror)\n",
        "        return 0.0\n",
        "    except Exception as e:\n",
        "        # unexpected error during sdr calculation\n",
        "        traceback.print_exc() # print full error details\n",
        "        return 0.0\n",
        "\n",
        "# function: evaluate_separation_model\n",
        "def evaluate_separation_model(model, test_loader, device):\n",
        "    \"\"\"Evaluates the separation model on the test set using SDR.\"\"\"\n",
        "    model.eval()\n",
        "    total_sdr = 0.0\n",
        "    valid_sdr_count = 0 # use a separate counter for valid sdr scores\n",
        "    processed_items = 0\n",
        "    print(\"Evaluating separation model on test set...\")\n",
        "    if not test_loader:\n",
        "         print(\"Test separation loader not available. Skipping evaluation.\")\n",
        "         return 0.0\n",
        "\n",
        "    progress_bar = tqdm(test_loader, desc=\"Separation Eval\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            if batch is None or len(batch) < 4: continue\n",
        "            mixture_mag, _, mixture_phase, clean_music_chunk = batch\n",
        "            if mixture_mag is None or mixture_phase is None or clean_music_chunk is None: continue\n",
        "\n",
        "            mixture_mag_dev = mixture_mag.to(device)\n",
        "            mixture_phase_dev = mixture_phase.to(device)\n",
        "\n",
        "            if mixture_mag_dev.dim() == 3: mixture_mag_dev = mixture_mag_dev.unsqueeze(1)\n",
        "\n",
        "            predicted_mask = model(mixture_mag_dev)\n",
        "\n",
        "            if predicted_mask.shape != mixture_mag_dev.shape:\n",
        "                target_h, target_w = mixture_mag_dev.shape[2], mixture_mag_dev.shape[3]\n",
        "                predicted_mask = F.interpolate(predicted_mask, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "\n",
        "            for i in range(mixture_mag_dev.size(0)):\n",
        "                processed_items += 1\n",
        "                input_mag_i = mixture_mag_dev[i].squeeze(0)\n",
        "                mask_i = predicted_mask[i].squeeze(0)\n",
        "                phase_i = mixture_phase_dev[i].squeeze(0)\n",
        "\n",
        "                if input_mag_i.shape != mask_i.shape: continue # skip if shapes mismatch\n",
        "\n",
        "                est_mag_i = input_mag_i * mask_i\n",
        "                true_wav_i = clean_music_chunk[i].cpu()\n",
        "\n",
        "                try:\n",
        "                    # assuming waveform_from_spectrogram is defined elsewhere\n",
        "                    est_wav_i = waveform_from_spectrogram(est_mag_i, phase_i)\n",
        "                    if est_wav_i is None:\n",
        "                         continue # skip if reconstruction fails\n",
        "\n",
        "                    est_wav_i = est_wav_i.cpu()\n",
        "                    # ensure estimated waveform length matches true waveform length\n",
        "                    est_wav_i = est_wav_i[:len(true_wav_i)]\n",
        "\n",
        "                    sdr = calculate_sdr(est_wav_i, true_wav_i) # calculate sdr using modified function\n",
        "\n",
        "                    # only accumulate and count non-zero and non-nan sdrs\n",
        "                    if not np.isnan(sdr) and sdr != 0.0:\n",
        "                        total_sdr += sdr\n",
        "                        valid_sdr_count += 1\n",
        "                    # else: # optional: track zero/nan sdrs\n",
        "                        # pass\n",
        "\n",
        "                except Exception as recon_err:\n",
        "                    print(f\"Error during waveform reconstruction or SDR for item {i} in batch {batch_idx}: {recon_err}\")\n",
        "                    traceback.print_exc() # print full error details\n",
        "                    continue\n",
        "\n",
        "            # update progress bar description\n",
        "            if valid_sdr_count > 0:\n",
        "                progress_bar.set_description(f\"Separation Eval (Avg SDR: {total_sdr / valid_sdr_count:.2f})\")\n",
        "            else:\n",
        "                progress_bar.set_description(f\"Separation Eval (Avg SDR: N/A)\")\n",
        "\n",
        "\n",
        "    if valid_sdr_count == 0:\n",
        "        print(f\"--- No valid (non-zero, non-NaN) SDR scores calculated out of {processed_items} items processed. Check for errors above. ---\")\n",
        "        return 0.0\n",
        "\n",
        "    avg_sdr = total_sdr / valid_sdr_count\n",
        "    print(f\"\\n--- Average SDR on Test Set (from {valid_sdr_count} valid scores): {avg_sdr:.4f} ---\")\n",
        "    return avg_sdr\n",
        "\n",
        "\n",
        "# function: evaluate_classification_model\n",
        "def evaluate_classification_model(model, test_loader, device):\n",
        "    \"\"\"Evaluates the classification model on the test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    print(\"Evaluating classification model on test set...\")\n",
        "    if not test_loader:\n",
        "         print(\"Test genre loader not available. Skipping evaluation.\")\n",
        "         return {}\n",
        "    progress_bar = tqdm(test_loader, desc=\"Classification Eval\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            if batch is None or len(batch) < 2: continue\n",
        "            inputs, targets = batch[0], batch[1] # log_mel_spec, label\n",
        "            if inputs is None or targets is None or inputs.nelement() == 0 or targets.nelement() == 0: continue\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            if inputs.dim() == 3: inputs = inputs.unsqueeze(1)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "    if not all_targets: print(\"No targets found for classification evaluation.\"); return {}\n",
        "    # assuming GENRE_CLASSES and NUM_GENRES are defined globally or passed differently\n",
        "    target_names_cls = GENRE_CLASSES if 'GENRE_CLASSES' in locals() else [str(i) for i in range(NUM_GENRES)]\n",
        "    present_labels = sorted(np.unique(all_targets + all_preds))\n",
        "    valid_present_labels = [l for l in present_labels if l >= 0 and l < len(target_names_cls)]\n",
        "    filtered_target_names = [target_names_cls[i] for i in valid_present_labels]\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    f1 = f1_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    if not filtered_target_names: print(\"Cannot generate classification report: No valid target names found.\"); report = \"N/A\"; conf_matrix = \"N/A\"\n",
        "    else:\n",
        "         report = classification_report(all_targets, all_preds, labels=valid_present_labels, target_names=filtered_target_names, zero_division=0); print(report)\n",
        "         try:\n",
        "             print(\"\\n--- Confusion Matrix ---\"); cm = confusion_matrix(all_targets, all_preds, labels=valid_present_labels); print(f\"Labels: {filtered_target_names}\"); print(cm); conf_matrix = cm\n",
        "         except Exception as cm_err: print(f\"Could not generate confusion matrix: {cm_err}\"); conf_matrix = \"N/A\"\n",
        "    print(f\"--- Overall Accuracy: {accuracy:.4f} ---\"); print(f\"--- Weighted F1-Score: {f1:.4f} ---\")\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1, \"report\": report, \"conf_matrix\": conf_matrix}\n",
        "\n"
      ],
      "metadata": {
        "id": "iwgk_YDfqBiB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NYNsaYeHZlA"
      },
      "source": [
        "# 7. Combination Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ObQb4NcCaPeN"
      },
      "outputs": [],
      "source": [
        "# combination logic\n",
        "# defines function to separate then classify audio\n",
        "# includes interpolation fix for shape mismatch\n",
        "# ensure helpers (load_audio, get_spectrogram, waveform_from_spectrogram)\n",
        "# and variables (sample_rate, audio_chunk_samples, n_fft, hop_length, device, num_genres, genre_classes)\n",
        "# are defined and accessible\n",
        "\n",
        "# function: separate_and_classify\n",
        "def separate_and_classify(mixture_waveform_path, separation_model, classification_model, device):\n",
        "    \"\"\"Applies separation then classification (Sequential Approach) to an audio file path.\"\"\"\n",
        "    # set models to evaluation mode\n",
        "    separation_model.eval()\n",
        "    classification_model.eval()\n",
        "\n",
        "    try:\n",
        "        # load mixture waveform\n",
        "        # assumes load_audio defined (returns cpu tensor)\n",
        "        mixture_waveform = load_audio(mixture_waveform_path, target_sr=SAMPLE_RATE)\n",
        "        if mixture_waveform is None:\n",
        "            print(f\"Error: Failed to load audio file {mixture_waveform_path}\")\n",
        "            return None, None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # separation step\n",
        "            # 1. get spectrogram (cpu)\n",
        "            # assumes get_spectrogram defined\n",
        "            mixture_mag_cpu, mixture_phase_cpu = get_spectrogram(mixture_waveform)\n",
        "\n",
        "            # 2. prepare for model (add dims, move to device)\n",
        "            mixture_mag_dev = mixture_mag_cpu.unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "            # 3. predict mask\n",
        "            predicted_mask = separation_model(mixture_mag_dev) # output on device\n",
        "\n",
        "            # interpolate mask to match input shape\n",
        "            if predicted_mask.shape != mixture_mag_dev.shape:\n",
        "                 target_h, target_w = mixture_mag_dev.shape[2], mixture_mag_dev.shape[3] # get target shape from input mag\n",
        "                 predicted_mask = F.interpolate(predicted_mask, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "            # end interpolation\n",
        "\n",
        "            # remove batch/channel dims for reconstruction\n",
        "            input_mag_i = mixture_mag_dev.squeeze(0).squeeze(0) # shape [freq, time] on device\n",
        "            mask_i = predicted_mask.squeeze(0).squeeze(0)      # shape [freq, time] on device\n",
        "\n",
        "            if input_mag_i.shape != mask_i.shape:\n",
        "                 print(f\"Error: Shape mismatch even after interpolation! Input: {input_mag_i.shape}, Mask: {mask_i.shape}\")\n",
        "                 return None, None # cannot proceed\n",
        "\n",
        "            # 4. apply mask\n",
        "            estimated_music_mag = input_mag_i * mask_i # result on device\n",
        "\n",
        "            # 5. reconstruct waveform\n",
        "            # assumes waveform_from_spectrogram defined\n",
        "            separated_music_waveform = waveform_from_spectrogram(estimated_music_mag, mixture_phase_cpu.to(device))\n",
        "            separated_music_waveform = separated_music_waveform.cpu() # move result to cpu\n",
        "            separated_music_waveform = separated_music_waveform[:mixture_waveform.shape[0]] # trim length\n",
        "\n",
        "\n",
        "            # classification step (on separated waveform)\n",
        "            required_samples = AUDIO_CHUNK_SAMPLES\n",
        "            if len(separated_music_waveform) >= required_samples:\n",
        "                 start = (len(separated_music_waveform) - required_samples) // 2\n",
        "                 class_chunk = separated_music_waveform[start : start + required_samples]\n",
        "            else:\n",
        "                 padding = required_samples - len(separated_music_waveform)\n",
        "                 class_chunk = torch.nn.functional.pad(separated_music_waveform, (0, padding))\n",
        "\n",
        "            # extract features (mel spectrogram)\n",
        "            # ensure config variables (sample_rate, n_fft, hop_length) are available\n",
        "            mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=128 # assuming n_mels=128\n",
        "            ).to(device)\n",
        "            log_mel_spec_transform = torchaudio.transforms.AmplitudeToDB().to(device)\n",
        "\n",
        "            mel_spec = mel_spectrogram_transform(class_chunk.to(device))\n",
        "            log_mel_spec = log_mel_spec_transform(mel_spec)\n",
        "            log_mel_spec = log_mel_spec.unsqueeze(0).unsqueeze(0) # add batch/channel dims\n",
        "\n",
        "            # classify\n",
        "            logits = classification_model(log_mel_spec)\n",
        "            prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # return prediction index and separated waveform (cpu)\n",
        "        return prediction, separated_music_waveform\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during separate_and_classify for {mixture_waveform_path}: {e}\")\n",
        "        traceback.print_exc() # print full traceback\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Training execution function"
      ],
      "metadata": {
        "id": "XBYW5WZ3NCaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main execution block for training\n",
        "# trains task 1 fresh (5s chunks, bs=32, lr scheduler)\n",
        "# trains task 2 (weight decay, early stopping)\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# lr scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import soundfile as sf\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import traceback # for detailed errors\n",
        "\n",
        "# record start time\n",
        "overall_start_time = time.time()\n",
        "\n",
        "print(\"=============================================\")\n",
        "print(\"=== Starting Project Execution (Final Run Attempt) ===\")\n",
        "# ensure config variables defined (from cell [27])\n",
        "# check existence using locals()/globals()\n",
        "config_vars_ok = all(var in locals() or var in globals() for var in ['device', 'AUDIO_CHUNK_DURATION_S', 'BATCH_SIZE', 'NUM_EPOCHS_SEP', 'NUM_EPOCHS_CLS', 'LEARNING_RATE_SEP', 'LEARNING_RATE_CLS', 'OUTPUT_DIR', 'NUM_GENRES', 'GENRE_CLASSES', 'SAMPLE_RATE', 'N_FFT', 'HOP_LENGTH'])\n",
        "if not config_vars_ok:\n",
        "     print(\"!!! ERROR: Essential config variables missing.\")\n",
        "     print(\"!!! Run Configuration Cell [27] first.\")\n",
        "     raise NameError(\"Configuration variables missing.\")\n",
        "else:\n",
        "    print(f\"=== Using Device: {device} ===\")\n",
        "    print(f\"=== Target Separation Chunks: {AUDIO_CHUNK_DURATION_S}s ===\")\n",
        "    print(f\"=== Target Batch Size: {BATCH_SIZE} ===\")\n",
        "print(\"=============================================\")\n",
        "\n",
        "# configuration: set to true to run task 1 training\n",
        "TRAIN_TASK_1 = True\n",
        "\n",
        "# initialize models\n",
        "print(\"\\nInitializing models...\")\n",
        "try:\n",
        "    # assumes model classes defined\n",
        "    separation_model = SeparationUNet().to(device)\n",
        "    classification_model = get_pretrained_genre_classifier(num_genres=NUM_GENRES, pretrained=True).to(device)\n",
        "    print(\"Models initialized.\")\n",
        "except NameError as ne:\n",
        "    print(f\"!!! ERROR: Model class definition not found ({ne}). Run Cell [29].\")\n",
        "    raise\n",
        "\n",
        "# set up loss functions and optimizers\n",
        "print(\"\\nSetting up loss functions and optimizers...\")\n",
        "try:\n",
        "    # assumes learning rates defined (in cell [27])\n",
        "    criterion_sep = nn.L1Loss()\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "    # task 1 optimizer & lr scheduler\n",
        "    optimizer_sep = optim.Adam(separation_model.parameters(), lr=LEARNING_RATE_SEP)\n",
        "    scheduler_sep = ReduceLROnPlateau(optimizer_sep, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-7)\n",
        "    print(f\"Optimizer_sep: Adam, LR={LEARNING_RATE_SEP}, Scheduler=ReduceLROnPlateau(patience=5)\")\n",
        "\n",
        "    # task 2 optimizer (with weight decay)\n",
        "    weight_decay_cls = 1e-4\n",
        "    optimizer_cls = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE_CLS, weight_decay=weight_decay_cls)\n",
        "    print(f\"Optimizer_cls: LR={LEARNING_RATE_CLS}, Weight Decay={weight_decay_cls}\")\n",
        "\n",
        "except Exception as e:\n",
        "     print(f\"!!! ERROR setting up loss/optimizers: {e}\")\n",
        "     raise\n",
        "\n",
        "# check if dataloaders created\n",
        "# assumes cell [34] (data handling) was run after cell [27]\n",
        "loaders_available = all([\n",
        "    'train_sep_loader' in locals() and train_sep_loader is not None,\n",
        "    'val_sep_loader' in locals() and val_sep_loader is not None,\n",
        "    'test_sep_loader' in locals() and test_sep_loader is not None,\n",
        "    'train_genre_loader' in locals() and train_genre_loader is not None,\n",
        "    'val_genre_loader' in locals() and val_genre_loader is not None,\n",
        "    'test_genre_loader' in locals() and test_genre_loader is not None\n",
        "])\n",
        "\n",
        "if not loaders_available:\n",
        "    print(\"\\n!!! ERROR: DataLoaders not available. Run Cell [34] after updating Config Cell [27].\")\n",
        "    raise ValueError(\"DataLoaders missing, cannot proceed.\")\n",
        "elif 'separation_model' in locals() and 'classification_model' in locals():\n",
        "\n",
        "    # training phase: task 1 (separation - 5s chunks)\n",
        "    if TRAIN_TASK_1:\n",
        "        print(\"\\n--- Training Separation Model (Task 1 - 5s Chunks) ---\")\n",
        "\n",
        "        # starting fresh: delete old checkpoint if it exists\n",
        "        best_sep_model_path = os.path.join(OUTPUT_DIR, \"separation_model_best.pth\")\n",
        "        if os.path.exists(best_sep_model_path):\n",
        "            print(f\"Deleting previous checkpoint to start fresh: {best_sep_model_path}\")\n",
        "            try: os.remove(best_sep_model_path)\n",
        "            except Exception as e_del: print(f\"Warning: Could not delete {best_sep_model_path}: {e_del}\")\n",
        "        else:\n",
        "            print(\"No previous checkpoint found, starting fresh.\")\n",
        "\n",
        "        start_epoch_sep = 0\n",
        "        best_sep_val_loss = float('inf')\n",
        "        epochs_to_run_sep = NUM_EPOCHS_SEP # use value from cell [27]\n",
        "\n",
        "        print(f\"Starting training for {epochs_to_run_sep} epochs...\")\n",
        "        sep_train_losses = []\n",
        "        sep_val_losses = []\n",
        "\n",
        "        for epoch in range(epochs_to_run_sep):\n",
        "            current_epoch_display = start_epoch_sep + epoch + 1\n",
        "            print(f\"\\nEpoch {current_epoch_display}/{epochs_to_run_sep}\")\n",
        "            epoch_start_time = time.time()\n",
        "            try:\n",
        "                # assumes train/validate_epoch defined (cell [30])\n",
        "                train_loss = train_epoch(separation_model, train_sep_loader, criterion_sep, optimizer_sep, device, task_type='separation', epoch_num=current_epoch_display, total_epochs=epochs_to_run_sep)\n",
        "                val_metrics = validate_epoch(separation_model, val_sep_loader, criterion_sep, device, task_type='separation', epoch_num=current_epoch_display, total_epochs=epochs_to_run_sep)\n",
        "                val_loss = val_metrics['loss']\n",
        "\n",
        "                if np.isnan(train_loss) or np.isnan(val_loss): print(f\"!!! ERROR: NaN loss detected in Epoch {current_epoch_display}. Stopping.\"); break\n",
        "\n",
        "                sep_train_losses.append(train_loss); sep_val_losses.append(val_loss)\n",
        "                epoch_end_time = time.time()\n",
        "                print(f\"  Epoch {current_epoch_display} Summary: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}, Time={epoch_end_time - epoch_start_time:.2f}s\")\n",
        "\n",
        "                if val_loss < best_sep_val_loss:\n",
        "                    best_sep_val_loss = val_loss\n",
        "                    save_path = os.path.join(OUTPUT_DIR, \"separation_model_best.pth\")\n",
        "                    try: torch.save(separation_model.state_dict(), save_path); print(f\"  Saved best separation model (Val Loss: {best_sep_val_loss:.6f})\")\n",
        "                    except Exception as e: print(f\"  Error saving model: {e}\")\n",
        "\n",
        "                scheduler_sep.step(val_loss) # step the lr scheduler\n",
        "\n",
        "            except NameError as e_func: print(f\"!!! ERROR: Training function not defined ({e_func}). Run Cell [30].\"); break\n",
        "            except RuntimeError as e_rt: print(f\"!!! RUNTIME ERROR in epoch {current_epoch_display}: {e_rt}\"); traceback.print_exc(); print(\"Try reducing BATCH_SIZE further (e.g., 16) in Cell [27] if this is OOM.\"); break\n",
        "            except Exception as e_gen: print(f\"!!! UNEXPECTED ERROR in epoch {current_epoch_display}: {e_gen}\"); traceback.print_exc(); break\n",
        "\n",
        "        print(\"\\nSeparation Model Training Finished.\")\n",
        "        if sep_train_losses: # plot task 1 results\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(range(1, len(sep_train_losses) + 1), sep_train_losses, label='Train Loss')\n",
        "            plt.plot(range(1, len(sep_val_losses) + 1), sep_val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch'); plt.ylabel('Loss (L1)'); plt.title('Task 1: Separation Model Training Curve (5s Chunks)')\n",
        "            plt.legend(); plt.grid(True)\n",
        "            plot_save_path = os.path.join(OUTPUT_DIR, 'separation_training_curve_5s.png')\n",
        "            plt.savefig(plot_save_path); print(f\"Saved training curve plot to {plot_save_path}\")\n",
        "\n",
        "    # load best separation model checkpoint\n",
        "    best_sep_model_path = os.path.join(OUTPUT_DIR, \"separation_model_best.pth\")\n",
        "    if os.path.exists(best_sep_model_path):\n",
        "        print(f\"\\nLoading best separation model for evaluation from: {best_sep_model_path}\")\n",
        "        try: separation_model.load_state_dict(torch.load(best_sep_model_path, map_location=device)); separation_model.eval(); print(\"Successfully loaded BEST separation model checkpoint.\")\n",
        "        except Exception as e: print(f\"!!! ERROR loading final best separation model checkpoint: {e} !!!\"); separation_model.eval()\n",
        "    else: print(f\"\\n!!! WARNING: Best separation model checkpoint not found. Using potentially untrained model! !!!\"); separation_model.eval()\n",
        "\n",
        "    # training classification model (task 2 - weight decay & early stopping)\n",
        "    print(\"\\nRe-initializing classification model for fresh training...\")\n",
        "    classification_model = get_pretrained_genre_classifier(num_genres=NUM_GENRES, pretrained=True).to(device)\n",
        "    optimizer_cls = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE_CLS, weight_decay=weight_decay_cls)\n",
        "\n",
        "    best_cls_model_path = os.path.join(OUTPUT_DIR, \"classification_model_best.pth\")\n",
        "    # delete previous checkpoint\n",
        "    if os.path.exists(best_cls_model_path):\n",
        "        print(f\"Deleting previous classification checkpoint: {best_cls_model_path}\")\n",
        "        try:\n",
        "            os.remove(best_cls_model_path)\n",
        "        except Exception as e_del_cls:\n",
        "            print(f\"Warning: Could not delete {best_cls_model_path}: {e_del_cls}\")\n",
        "\n",
        "    print(\"\\n--- Training Classification Model (Task 2) ---\")\n",
        "    print(f\"Starting training for up to {NUM_EPOCHS_CLS} epochs (with Early Stopping)...\")\n",
        "    best_cls_val_f1 = -1.0; epochs_no_improve_cls = 0; patience_cls = 7\n",
        "    cls_train_losses, cls_val_losses, cls_val_accuracies, cls_val_f1s = [], [], [], []\n",
        "    for epoch in range(NUM_EPOCHS_CLS):\n",
        "        current_epoch_display = epoch + 1; print(f\"\\nEpoch {current_epoch_display}/{NUM_EPOCHS_CLS}\"); epoch_start_time = time.time()\n",
        "        try:\n",
        "            train_loss = train_epoch(classification_model, train_genre_loader, criterion_cls, optimizer_cls, device, task_type='classification', epoch_num=current_epoch_display, total_epochs=NUM_EPOCHS_CLS)\n",
        "            val_metrics = validate_epoch(classification_model, val_genre_loader, criterion_cls, device, task_type='classification', epoch_num=current_epoch_display, total_epochs=NUM_EPOCHS_CLS)\n",
        "            val_loss = val_metrics['loss']; val_accuracy = val_metrics.get('accuracy', 0.0); val_f1 = val_metrics.get('f1', 0.0)\n",
        "            if np.isnan(train_loss) or np.isnan(val_loss): print(f\"!!! ERROR: NaN loss detected in Epoch {current_epoch_display}. Stopping.\"); break\n",
        "            cls_train_losses.append(train_loss); cls_val_losses.append(val_loss); cls_val_accuracies.append(val_accuracy); cls_val_f1s.append(val_f1)\n",
        "            epoch_end_time = time.time()\n",
        "            print(f\"  Epoch {current_epoch_display}/{NUM_EPOCHS_CLS} Summary: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}, Val F1={val_f1:.4f}, Time={epoch_end_time - epoch_start_time:.2f}s\")\n",
        "            # early stopping check\n",
        "            if val_f1 > best_cls_val_f1:\n",
        "                best_cls_val_f1 = val_f1; epochs_no_improve_cls = 0\n",
        "                save_path = os.path.join(OUTPUT_DIR, \"classification_model_best.pth\")\n",
        "                try: torch.save(classification_model.state_dict(), save_path); print(f\"  Saved best classification model (F1: {val_f1:.4f})\")\n",
        "                except Exception as e: print(f\"  Error saving model: {e}\")\n",
        "            else:\n",
        "                epochs_no_improve_cls += 1; print(f\"  Validation F1 did not improve for {epochs_no_improve_cls} epoch(s). Best: {best_cls_val_f1:.4f}\")\n",
        "            if epochs_no_improve_cls >= patience_cls: print(f\"\\nEarly stopping triggered.\"); break\n",
        "        except NameError as e_func: print(f\"!!! ERROR: Training function not defined ({e_func}). Run Cell [30].\"); break\n",
        "        except Exception as e_gen: print(f\"!!! UNEXPECTED ERROR in epoch {current_epoch_display}: {e_gen}\"); traceback.print_exc(); break\n",
        "    print(\"\\nClassification Model Training Finished.\")\n",
        "    # plot task 2 results\n",
        "    if cls_train_losses:\n",
        "        epochs_trained_cls = len(cls_train_losses); fig, ax1 = plt.subplots(figsize=(12, 5)); color='tab:red'; ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss', color=color); ax1.plot(range(1, epochs_trained_cls + 1), cls_train_losses, '--', label='Train Loss', color=color); ax1.plot(range(1, epochs_trained_cls + 1), cls_val_losses, label='Validation Loss', color=color); ax1.tick_params(axis='y', labelcolor=color); ax1.legend(loc='upper left'); ax1.grid(True); ax2 = ax1.twinx(); color='tab:blue'; ax2.set_ylabel('Metrics (F1/Acc)', color=color); ax2.plot(range(1, epochs_trained_cls + 1), cls_val_accuracies, ':', label='Validation Accuracy', color=color); ax2.plot(range(1, epochs_trained_cls + 1), cls_val_f1s, label='Validation F1', color=color); ax2.tick_params(axis='y', labelcolor=color); ax2.set_ylim(bottom=0); ax2.legend(loc='upper right'); fig.tight_layout(); plt.title('Task 2: Classification Model Training Curve (Regularized)'); plot_save_path_cls = os.path.join(OUTPUT_DIR, 'classification_training_curve_regularized_final.png'); plt.savefig(plot_save_path_cls); print(f\"Saved training curve plot to {plot_save_path_cls}\")\n",
        "\n",
        "    # load best classification model\n",
        "    best_cls_model_path = os.path.join(OUTPUT_DIR, \"classification_model_best.pth\")\n",
        "    if os.path.exists(best_cls_model_path):\n",
        "        print(f\"\\nLoading best classification model (based on Val F1) from: {best_cls_model_path}\")\n",
        "        try:\n",
        "            classification_model.load_state_dict(torch.load(best_cls_model_path, map_location=device))\n",
        "            classification_model.eval()\n",
        "            print(\"Successfully loaded BEST classification model checkpoint.\")\n",
        "        except Exception as e:\n",
        "            print(f\"!!! ERROR loading best classification model checkpoint: {e} !!!\")\n",
        "            if 'classification_model' in locals() and hasattr(classification_model, 'eval'): classification_model.eval()\n",
        "    else:\n",
        "        print(f\"\\n!!! WARNING: Best classification model checkpoint not found at {best_cls_model_path}. Using current model state. !!!\")\n",
        "        if 'classification_model' in locals() and hasattr(classification_model, 'eval'): classification_model.eval()\n",
        "\n",
        "\n",
        "    # evaluation phase\n",
        "    separation_model.eval(); classification_model.eval()\n",
        "    print(\"\\n--- Evaluating Models on Test Set ---\")\n",
        "    print(\"\\nEvaluating Separation Model (SDR)...\")\n",
        "    try: avg_sdr = evaluate_separation_model(separation_model, test_sep_loader, device)\n",
        "    except Exception as e_eval_sep: print(f\"Error during separation eval: {e_eval_sep}\"); avg_sdr = \"N/A\"\n",
        "    print(\"\\nEvaluating Classification Model (Baseline)...\")\n",
        "    try: baseline_cls_results = evaluate_classification_model(classification_model, test_genre_loader, device)\n",
        "    except Exception as e_eval_cls: print(f\"Error during classification eval: {e_eval_cls}\"); baseline_cls_results = {}\n",
        "\n",
        "    # combination evaluation\n",
        "    print(\"\\nEvaluating Combined Model (Sequential)...\")\n",
        "    combined_test_files = genre_files_test if 'genre_files_test' in locals() else None\n",
        "    combined_test_labels = genre_labels_test if 'genre_labels_test' in locals() else None\n",
        "    all_combined_preds = []; all_combined_targets = []\n",
        "    if combined_test_files and combined_test_labels:\n",
        "        print(f\"Processing {len(combined_test_files)} files for combined evaluation...\")\n",
        "        try:\n",
        "            for i, file_path in enumerate(combined_test_files):\n",
        "                true_label = combined_test_labels[i]\n",
        "                pred_label, _ = separate_and_classify(file_path, separation_model, classification_model, device)\n",
        "                if pred_label is not None: all_combined_preds.append(pred_label); all_combined_targets.append(true_label)\n",
        "                if (i + 1) % 50 == 0: print(f\"  Processed {i+1}/{len(combined_test_files)} for combined eval...\")\n",
        "        except NameError as func_err: print(f\"!!! ERROR: Required function not defined ({func_err}). Make sure Cell [32] was run.\")\n",
        "        except Exception as e_comb_eval: print(f\"Error during combined eval processing: {e_comb_eval}\")\n",
        "    if all_combined_targets:\n",
        "        combined_accuracy = accuracy_score(all_combined_targets, all_combined_preds); combined_f1 = f1_score(all_combined_targets, all_combined_preds, average='weighted', zero_division=0); print(\"\\n--- Combined (Sequential) Classification Report ---\"); target_names_cls = GENRE_CLASSES if 'GENRE_CLASSES' in locals() else [str(i) for i in range(NUM_GENRES)]; present_labels = sorted(np.unique(all_combined_targets + all_combined_preds)); valid_present_labels = [l for l in present_labels if l >= 0 and l < len(target_names_cls)]; filtered_target_names = [target_names_cls[i] for i in valid_present_labels];\n",
        "        if not filtered_target_names: print(\"No valid predictions/targets for classification report.\")\n",
        "        else: print(classification_report(all_combined_targets, all_combined_preds, labels=valid_present_labels, target_names=filtered_target_names, zero_division=0))\n",
        "        print(f\"--- Combined Accuracy: {combined_accuracy:.4f} ---\"); print(f\"--- Combined Weighted F1-Score: {combined_f1:.4f} ---\"); baseline_acc = baseline_cls_results.get('accuracy', -1); baseline_f1 = baseline_cls_results.get('f1', -1); print(f\"\\nComparison: Baseline Acc={baseline_acc:.4f}, F1={baseline_f1:.4f} | Combined Acc={combined_accuracy:.4f}, F1={combined_f1:.4f}\")\n",
        "    else: print(\"Could not perform combined evaluation (no valid targets/predictions).\")\n",
        "\n",
        "\n",
        "    # case study\n",
        "    print(\"\\n--- Processing Case Study File ---\")\n",
        "    if 'CASE_STUDY_FILE' in locals() and os.path.exists(CASE_STUDY_FILE):\n",
        "        print(f\"Loading case study file: {CASE_STUDY_FILE}\")\n",
        "        # classify original case study audio\n",
        "        print(\"Classifying original case study audio...\")\n",
        "        original_pred = None\n",
        "        try:\n",
        "            case_study_waveform = load_audio(CASE_STUDY_FILE, target_sr=SAMPLE_RATE)\n",
        "            if case_study_waveform is not None:\n",
        "                required_samples = AUDIO_CHUNK_SAMPLES\n",
        "                if len(case_study_waveform) >= required_samples:\n",
        "                    start = (len(case_study_waveform) - required_samples) // 2\n",
        "                    case_chunk = case_study_waveform[start : start + required_samples]\n",
        "                else:\n",
        "                    padding = required_samples - len(case_study_waveform)\n",
        "                    case_chunk = torch.nn.functional.pad(case_study_waveform, (0, padding))\n",
        "                mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=128).to(device)\n",
        "                log_mel_spec_transform = torchaudio.transforms.AmplitudeToDB().to(device)\n",
        "                mel_spec = mel_spectrogram_transform(case_chunk.to(device))\n",
        "                log_mel_spec = log_mel_spec_transform(mel_spec).unsqueeze(0).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    logits = classification_model(log_mel_spec)\n",
        "                    original_pred = torch.argmax(logits, dim=1).item()\n",
        "                print(f\"  Original Prediction: {GENRE_CLASSES[original_pred] if original_pred is not None and original_pred < len(GENRE_CLASSES) else 'Error/Unknown'}\")\n",
        "            else:\n",
        "                print(\"  Failed to load case study waveform.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error classifying original: {e}\")\n",
        "            traceback.print_exc() # print full traceback\n",
        "\n",
        "        print(\"\\nSeparating and Classifying case study audio...\")\n",
        "        try:\n",
        "            combined_pred, separated_wav = separate_and_classify(CASE_STUDY_FILE, separation_model, classification_model, device)\n",
        "            if combined_pred is not None and separated_wav is not None:\n",
        "                print(f\"  Combined Prediction: {GENRE_CLASSES[combined_pred] if combined_pred < len(GENRE_CLASSES) else 'Unknown'}\")\n",
        "                separated_filename = os.path.join(OUTPUT_DIR, f\"{os.path.splitext(os.path.basename(CASE_STUDY_FILE))[0]}_separated_classified_final.wav\")\n",
        "                try:\n",
        "                    if separated_wav.is_cuda or hasattr(separated_wav, 'is_mps') and separated_wav.is_mps: separated_wav = separated_wav.cpu()\n",
        "                    save_data = separated_wav.to(torch.float32).numpy()\n",
        "                    if save_data.ndim > 1: save_data = save_data.squeeze()\n",
        "                    sf.write(separated_filename, save_data, SAMPLE_RATE)\n",
        "                    print(f\"  Saved separated audio to {separated_filename}\")\n",
        "                except Exception as e: print(f\"  Error saving separated audio: {e}\")\n",
        "            else: print(\"  Failed to process combined.\")\n",
        "        except NameError: print(\"separate_and_classify function not found. Make sure Cell [32] was run.\")\n",
        "        except Exception as e_sep_cls: print(f\"  Error during combined processing: {e_sep_cls}\"); traceback.print_exc() # print full traceback\n",
        "    else:\n",
        "        print(f\"Case study file not found or not defined: {locals().get('CASE_STUDY_FILE', 'Not Defined')}\")\n",
        "\n",
        "else:\n",
        "     print(\"!!! Skipping Run because DataLoaders or Models are missing. Check previous cells.\")\n",
        "\n",
        "\n",
        "overall_end_time = time.time()\n",
        "print(\"\\n=============================================\")\n",
        "print(\"=== Project Execution Finished ===\")\n",
        "if 'overall_start_time' in locals(): print(f\"Total execution time: {(overall_end_time - overall_start_time)/60:.2f} minutes\")\n",
        "else: print(\"Total execution time not calculated.\")\n",
        "print(\"=============================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a1d18f23223846c79cb8a062fc458241",
            "1b0e49ab9587427187c5a6872c9d1279",
            "2f53fab508b54fdfbead493a19d29c59",
            "d272fed9f32f4694b6f7aadd01219d3f",
            "7757682b98db449da1fd8e6565c84571",
            "66aa94668b124984a914b720912fb5b0",
            "75e8923ece3b4866a79ddeb3df8a02c4",
            "476d8e52c12045bc9470323ed98f9b05",
            "b02b3b25c0a24c75b08832d3858604f6",
            "f01dcd83e6624e32b8c92472b7d690a1",
            "88c17211e6bf42fdb362fb97e01c987f"
          ]
        },
        "collapsed": true,
        "id": "dWuck1Pw1cLf",
        "outputId": "fd4f3e73-2d31-4d78-f410-5f6fc3bc7fb7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "=== Starting Project Execution (Final Run Attempt) ===\n",
            "=== Using Device: cuda ===\n",
            "=== Target Separation Chunks: 5s ===\n",
            "=== Target Batch Size: 32 ===\n",
            "=============================================\n",
            "\n",
            "Initializing models...\n",
            "U-Net Encoder features: [16, 32, 64, 128, 256]\n",
            "U-Net Bottleneck features: 512\n",
            "U-Net Decoder features (output channels per block): [256, 128, 64, 32, 16]\n",
            "Loading pre-trained ResNet18 model...\n",
            "Adapted model.conv1 to accept 1 input channel.\n",
            "Initialized new conv1 weights by averaging original RGB weights.\n",
            "Adapted model.fc to output 8 classes.\n",
            "Models initialized.\n",
            "\n",
            "Setting up loss functions and optimizers...\n",
            "Optimizer_sep: Adam, LR=1e-05, Scheduler=ReduceLROnPlateau(patience=5)\n",
            "Optimizer_cls: LR=0.001, Weight Decay=0.0001\n",
            "\n",
            "--- Training Separation Model (Task 1 - 5s Chunks) ---\n",
            "Deleting previous checkpoint to start fresh: /content/drive/MyDrive/cw1_DL/output/separation_model_best.pth\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/30 [Separation Train]:   0%|          | 0/525 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1d18f23223846c79cb8a062fc458241"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolated output shape to match target: torch.Size([32, 1, 1025, 431])\n",
            "\n",
            "--- Debug Info (Epoch 1, Batch 0) ---\n",
            "Task Type: separation\n",
            "Input shape: torch.Size([32, 1, 1025, 431]), Input Min: 0.0000, Max: 443.3140, Mean: 0.9287\n",
            "Target shape: torch.Size([32, 1, 1025, 431])\n",
            "Target Min: 0.0000, Max: 1.0000, Mean: 0.7087\n",
            "Output shape (potentially after interpolation): torch.Size([32, 1, 1025, 431])\n",
            "Output Min: 0.0000, Max: 1.0000, Mean: 0.5363\n",
            "Loss value: 0.419299\n",
            "--- End Debug Info ---\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-af6f2fa0acf5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# Assumes train_epoch/validate_epoch defined in Cell [30]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sep_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'separation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_epoch_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_to_run_sep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sep_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'separation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_epoch_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_to_run_sep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-71e9989e48e5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, task_type, epoch_num, total_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch_num}/{total_epochs} [{task_type.capitalize()} Train]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 Evaluation exectution function\n"
      ],
      "metadata": {
        "id": "0568heFDwTY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# record start time\n",
        "overall_start_time = time.time()\n",
        "\n",
        "print(\"=============================================\")\n",
        "print(\"=== Starting Project Execution (EVALUATION ONLY) ===\")\n",
        "# ensure config variables defined (from cell [27])\n",
        "config_vars_ok = all(var in locals() or var in globals() for var in ['device', 'AUDIO_CHUNK_DURATION_S', 'BATCH_SIZE', 'NUM_EPOCHS_SEP', 'NUM_EPOCHS_CLS', 'LEARNING_RATE_SEP', 'LEARNING_RATE_CLS', 'OUTPUT_DIR', 'NUM_GENRES', 'GENRE_CLASSES', 'SAMPLE_RATE', 'N_FFT', 'HOP_LENGTH', 'AUDIO_CHUNK_SAMPLES']) # Added AUDIO_CHUNK_SAMPLES check\n",
        "if not config_vars_ok:\n",
        "     print(\"!!! ERROR: Essential config variables missing.\")\n",
        "     print(\"!!! Run Configuration Cell [27] first.\")\n",
        "     raise NameError(\"Configuration variables missing.\")\n",
        "else:\n",
        "    print(f\"=== Using Device: {device} ===\")\n",
        "    print(f\"=== Target Separation Chunks (from training): {AUDIO_CHUNK_DURATION_S}s ===\")\n",
        "    print(f\"=== Target Batch Size (from training): {BATCH_SIZE} ===\")\n",
        "print(\"=============================================\")\n",
        "\n",
        "# initialize models (needed before loading weights)\n",
        "print(\"\\nInitializing model architectures...\")\n",
        "try:\n",
        "    # assumes model classes defined (cell [29])\n",
        "    separation_model = SeparationUNet().to(device)\n",
        "    classification_model = get_pretrained_genre_classifier(num_genres=NUM_GENRES, pretrained=False).to(device) # pretrained=false is fine when loading state_dict\n",
        "    print(\"Model architectures initialized.\")\n",
        "except NameError as ne:\n",
        "    print(f\"!!! ERROR: Model class definition not found ({ne}). Run Cell [29].\")\n",
        "    raise\n",
        "except Exception as e_init:\n",
        "     print(f\"!!! ERROR initializing models: {e_init}\")\n",
        "     raise\n",
        "\n",
        "# define paths to saved models\n",
        "# assumes output_dir defined (cell [27])\n",
        "best_sep_model_path = os.path.join(OUTPUT_DIR, \"separation_model_best.pth\")\n",
        "best_cls_model_path = os.path.join(OUTPUT_DIR, \"classification_model_best.pth\")\n",
        "\n",
        "# load best separation model checkpoint\n",
        "model_loaded_sep = False\n",
        "if os.path.exists(best_sep_model_path):\n",
        "    print(f\"\\nLoading best separation model for evaluation from: {best_sep_model_path}\")\n",
        "    try:\n",
        "        separation_model.load_state_dict(torch.load(best_sep_model_path, map_location=device))\n",
        "        separation_model.eval() # set to evaluation mode\n",
        "        print(\"Successfully loaded BEST separation model checkpoint.\")\n",
        "        model_loaded_sep = True\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR loading separation model checkpoint: {e} !!!\")\n",
        "else:\n",
        "    print(f\"\\n!!! WARNING: Best separation model checkpoint not found at {best_sep_model_path} !!!\")\n",
        "\n",
        "# load best classification model checkpoint\n",
        "model_loaded_cls = False\n",
        "if os.path.exists(best_cls_model_path):\n",
        "    print(f\"\\nLoading best classification model (based on Val F1) from: {best_cls_model_path}\")\n",
        "    try:\n",
        "        classification_model.load_state_dict(torch.load(best_cls_model_path, map_location=device))\n",
        "        classification_model.eval() # set to evaluation mode\n",
        "        print(\"Successfully loaded BEST classification model checkpoint.\")\n",
        "        model_loaded_cls = True\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR loading best classification model checkpoint: {e} !!!\")\n",
        "else:\n",
        "    print(f\"\\n!!! WARNING: Best classification model checkpoint not found at {best_cls_model_path} !!!\")\n",
        "\n",
        "\n",
        "# check if dataloaders created (test sets needed)\n",
        "# assumes cell [34] was run after cell [27]\n",
        "# only need test loaders\n",
        "loaders_available = all([\n",
        "    'test_sep_loader' in locals() and test_sep_loader is not None,\n",
        "    'test_genre_loader' in locals() and test_genre_loader is not None\n",
        "])\n",
        "\n",
        "if not loaders_available:\n",
        "    print(\"\\n!!! ERROR: Test DataLoaders not available. Run Cell [34] after Config Cell [27].\")\n",
        "    raise ValueError(\"Test DataLoaders missing, cannot proceed.\")\n",
        "\n",
        "# proceed only if models loaded and test loaders available\n",
        "elif model_loaded_sep and model_loaded_cls:\n",
        "\n",
        "    # evaluation phase\n",
        "    # ensure models are in eval mode\n",
        "    separation_model.eval()\n",
        "    classification_model.eval()\n",
        "    print(\"\\n--- Evaluating Models on Test Set ---\")\n",
        "\n",
        "    # evaluate separation model\n",
        "    print(\"\\nEvaluating Separation Model (SDR)...\")\n",
        "    avg_sdr = \"N/A\" # default value\n",
        "    try:\n",
        "        # assumes evaluate_separation_model defined (cell [31])\n",
        "        avg_sdr = evaluate_separation_model(separation_model, test_sep_loader, device)\n",
        "    except NameError:\n",
        "         print(\"evaluate_separation_model function not found. Run Cell [31].\")\n",
        "    except Exception as e_eval_sep:\n",
        "         print(f\"Error during separation eval: {e_eval_sep}\")\n",
        "         traceback.print_exc() # print traceback for eval errors\n",
        "\n",
        "    # evaluate classification model (baseline)\n",
        "    print(\"\\nEvaluating Classification Model (Baseline)...\")\n",
        "    baseline_cls_results = {} # default value\n",
        "    try:\n",
        "        # assumes evaluate_classification_model defined (cell [31])\n",
        "        baseline_cls_results = evaluate_classification_model(classification_model, test_genre_loader, device)\n",
        "    except NameError:\n",
        "         print(\"evaluate_classification_model function not found. Run Cell [31].\")\n",
        "    except Exception as e_eval_cls:\n",
        "         print(f\"Error during classification eval: {e_eval_cls}\")\n",
        "         traceback.print_exc()\n",
        "\n",
        "    # combination evaluation\n",
        "    print(\"\\nEvaluating Combined Model (Sequential)...\")\n",
        "    # assumes test files/labels defined (cell [34])\n",
        "    combined_test_files = genre_files_test if 'genre_files_test' in locals() else None\n",
        "    combined_test_labels = genre_labels_test if 'genre_labels_test' in locals() else None\n",
        "    all_combined_preds = []; all_combined_targets = []\n",
        "\n",
        "    if combined_test_files and combined_test_labels:\n",
        "        print(f\"Processing {len(combined_test_files)} files for combined evaluation...\")\n",
        "        try:\n",
        "            # assumes separate_and_classify defined (cell [32])\n",
        "            for i, file_path in enumerate(tqdm(combined_test_files, desc=\"Combined Eval\")): # add progress bar\n",
        "                true_label = combined_test_labels[i]\n",
        "                pred_label, _ = separate_and_classify(file_path, separation_model, classification_model, device)\n",
        "                if pred_label is not None:\n",
        "                    all_combined_preds.append(pred_label)\n",
        "                    all_combined_targets.append(true_label)\n",
        "                # optional: print progress less frequently\n",
        "        except NameError as func_err:\n",
        "             print(f\"!!! ERROR: Required function not defined ({func_err}). Make sure Cell [32] was run.\")\n",
        "        except Exception as e_comb_eval:\n",
        "            print(f\"Error during combined eval processing: {e_comb_eval}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # display combined results if generated\n",
        "        if all_combined_targets:\n",
        "            combined_accuracy = accuracy_score(all_combined_targets, all_combined_preds)\n",
        "            combined_f1 = f1_score(all_combined_targets, all_combined_preds, average='weighted', zero_division=0)\n",
        "            print(\"\\n--- Combined (Sequential) Classification Report ---\")\n",
        "            # assumes genre_classes defined (cell [27])\n",
        "            target_names_cls = GENRE_CLASSES if 'GENRE_CLASSES' in locals() else [str(i) for i in range(NUM_GENRES)]\n",
        "            present_labels = sorted(np.unique(all_combined_targets + all_combined_preds))\n",
        "            valid_present_labels = [l for l in present_labels if l >= 0 and l < len(target_names_cls)]\n",
        "            filtered_target_names = [target_names_cls[i] for i in valid_present_labels]\n",
        "\n",
        "            if not filtered_target_names: print(\"No valid predictions/targets for classification report.\")\n",
        "            else: print(classification_report(all_combined_targets, all_combined_preds, labels=valid_present_labels, target_names=filtered_target_names, zero_division=0))\n",
        "\n",
        "            print(f\"--- Combined Accuracy: {combined_accuracy:.4f} ---\")\n",
        "            print(f\"--- Combined Weighted F1-Score: {combined_f1:.4f} ---\")\n",
        "            baseline_acc = baseline_cls_results.get('accuracy', -1)\n",
        "            baseline_f1 = baseline_cls_results.get('f1', -1)\n",
        "            # ensure baseline results are numbers before formatting\n",
        "            baseline_acc_str = f\"{baseline_acc:.4f}\" if isinstance(baseline_acc, (int, float)) else \"N/A\"\n",
        "            baseline_f1_str = f\"{baseline_f1:.4f}\" if isinstance(baseline_f1, (int, float)) else \"N/A\"\n",
        "            print(f\"\\nComparison: Baseline Acc={baseline_acc_str}, F1={baseline_f1_str} | Combined Acc={combined_accuracy:.4f}, F1={combined_f1:.4f}\")\n",
        "        else:\n",
        "            print(\"Could not perform combined evaluation (no valid targets/predictions).\")\n",
        "    else:\n",
        "        print(\"Skipping combined evaluation - Test files/labels not found (check Cell [34] split).\")\n",
        "\n",
        "\n",
        "    # case study\n",
        "    print(\"\\n--- Processing Case Study File ---\")\n",
        "    # assumes case_study_file defined (cell [27])\n",
        "    if 'CASE_STUDY_FILE' in locals() and os.path.exists(CASE_STUDY_FILE):\n",
        "        print(f\"Loading case study file: {CASE_STUDY_FILE}\")\n",
        "        print(\"Classifying original case study audio...\")\n",
        "        original_pred = None\n",
        "        try:\n",
        "            # assumes load_audio, sample_rate, etc. defined\n",
        "            case_study_waveform = load_audio(CASE_STUDY_FILE, target_sr=SAMPLE_RATE)\n",
        "            if case_study_waveform is not None:\n",
        "                required_samples = AUDIO_CHUNK_SAMPLES\n",
        "                if len(case_study_waveform) >= required_samples: start = (len(case_study_waveform) - required_samples)//2; case_chunk = case_study_waveform[start : start + required_samples]\n",
        "                else: padding = required_samples - len(case_study_waveform); case_chunk = torch.nn.functional.pad(case_study_waveform, (0, padding))\n",
        "                mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=128).to(device); log_mel_spec_transform = torchaudio.transforms.AmplitudeToDB().to(device); mel_spec = mel_spectrogram_transform(case_chunk.to(device)); log_mel_spec = log_mel_spec_transform(mel_spec).unsqueeze(0).unsqueeze(0);\n",
        "                with torch.no_grad(): logits = classification_model(log_mel_spec); original_pred = torch.argmax(logits, dim=1).item();\n",
        "                print(f\"  Original Prediction: {GENRE_CLASSES[original_pred] if original_pred is not None and original_pred < len(GENRE_CLASSES) else 'Error/Unknown'}\")\n",
        "            else: print(\"  Failed to load case study waveform.\")\n",
        "        except Exception as e: print(f\"  Error classifying original: {e}\"); traceback.print_exc()\n",
        "\n",
        "        print(\"\\nSeparating and Classifying case study audio...\")\n",
        "        try:\n",
        "            # assumes separate_and_classify defined (cell [32])\n",
        "            combined_pred, separated_wav = separate_and_classify(CASE_STUDY_FILE, separation_model, classification_model, device)\n",
        "            if combined_pred is not None and separated_wav is not None:\n",
        "                print(f\"  Combined Prediction: {GENRE_CLASSES[combined_pred] if combined_pred < len(GENRE_CLASSES) else 'Unknown'}\")\n",
        "                # assumes output_dir defined\n",
        "                separated_filename = os.path.join(OUTPUT_DIR, f\"{os.path.splitext(os.path.basename(CASE_STUDY_FILE))[0]}_separated_classified_eval_only.wav\") # use different filename for eval output\n",
        "                try:\n",
        "                    if separated_wav.is_cuda or hasattr(separated_wav, 'is_mps') and separated_wav.is_mps: separated_wav = separated_wav.cpu()\n",
        "                    save_data = separated_wav.to(torch.float32).numpy();\n",
        "                    if save_data.ndim > 1: save_data = save_data.squeeze()\n",
        "                    # assumes sf, sample_rate defined\n",
        "                    sf.write(separated_filename, save_data, SAMPLE_RATE); print(f\"  Saved separated audio to {separated_filename}\")\n",
        "                except Exception as e: print(f\"  Error saving separated audio: {e}\")\n",
        "            else: print(\"  Failed to process combined.\")\n",
        "        except NameError: print(\"separate_and_classify function not found. Make sure Cell [32] was run.\")\n",
        "        except Exception as e_sep_cls: print(f\"  Error during combined processing: {e_sep_cls}\"); traceback.print_exc()\n",
        "    else: print(f\"Case study file not found or not defined: {locals().get('CASE_STUDY_FILE', 'Not Defined')}\")\n",
        "\n",
        "else:\n",
        "     print(\"!!! Skipping Evaluation because models could not be loaded or DataLoaders missing. Check previous cells and file paths.\")\n",
        "\n",
        "\n",
        "overall_end_time = time.time()\n",
        "print(\"\\n=============================================\")\n",
        "print(\"=== Project Execution Finished ===\")\n",
        "if 'overall_start_time' in locals(): print(f\"Total execution time: {(overall_end_time - overall_start_time)/60:.2f} minutes\")\n",
        "else: print(\"Total execution time not calculated.\")\n",
        "print(\"=============================================\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fe7b72c4b63e4b089948343c6dc1f309",
            "76a21e67fd864f2497263363a093ccd0",
            "e3fb3ddeb1054e068b26d9b3c10cd321",
            "9c5b96c29e1140a6a57bbda2a6e8afe3",
            "c4b6aa7e5eaa4a3c99c714d27b7d3ea8",
            "bb459b8899664e7398f26ac58b51584a",
            "4ea5382b985c4917abe63d36f098a2c8",
            "7a7af744ec3647d0b3f9408d658a4929",
            "a929299b37114d85835938f0690a50ad",
            "b2249e8c71d94742ad003492f272c56e",
            "03438b5f21d0438899491fe6f97570a2",
            "1926cec1342c46aaaa04b75cbff3958e",
            "5d91a1cb5843469c88b488fc38792722",
            "44f30a5bfa5c4160a746be53cc003bd2",
            "08cfabf4657f42199b274dfe6cd592ba",
            "a915e905c85440ddb642d183ed2bf2b2",
            "3e009ce0de184d6794bef4a32307735e",
            "2c3fdbabe0f04fd8a3729082f785bdc9",
            "00b6d52417604e828f9692e70b508b14",
            "9827f8e0bea246d0945bd911581ae199",
            "4beb416fc52f4cef909ab2dcd393911e",
            "5744afdbf91545e1b25d910fc37e5f72",
            "2ba61694983c4aa2af0025cf7c9644fe",
            "56068fa5fede49a1b9f7512589b94b6f",
            "423dbb8ca1fa4043800263529ece78a6",
            "2e86a8bf19944067909cbc31246b9f65",
            "7ac87150e4944ede83a989e5cd99857e",
            "b95782dc31c34acbb1462d8002f329cc",
            "55303a91b04a44af99935c8a9803a924",
            "bed5e375ee0a4174ab25407f48dfe0e7",
            "bbb110b3e02948d6b1c17a3ba2f9bc1b",
            "611a4604ceba4e8787583b6131ee214c",
            "442f572003b3466893b0e6c982993d17"
          ]
        },
        "id": "-Rau229MRYM3",
        "outputId": "8a2126be-0539-4377-f25f-6787579c871c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "=== Starting Project Execution (EVALUATION ONLY) ===\n",
            "=== Using Device: cuda ===\n",
            "=== Target Separation Chunks (from training): 5s ===\n",
            "=== Target Batch Size (from training): 32 ===\n",
            "=============================================\n",
            "\n",
            "Initializing model architectures...\n",
            "U-Net Encoder features: [16, 32, 64, 128, 256]\n",
            "U-Net Bottleneck features: 512\n",
            "U-Net Decoder features (output channels per block): [256, 128, 64, 32, 16]\n",
            "Loading randomly initialized ResNet18 model...\n",
            "Adapted model.conv1 to accept 1 input channel.\n",
            "Adapted model.fc to output 8 classes.\n",
            "Model architectures initialized.\n",
            "\n",
            "Loading best separation model for evaluation from: /content/drive/MyDrive/cw1_DL/output/separation_model_best.pth\n",
            "Successfully loaded BEST separation model checkpoint.\n",
            "\n",
            "Loading best classification model (based on Val F1) from: /content/drive/MyDrive/cw1_DL/output/classification_model_best.pth\n",
            "Successfully loaded BEST classification model checkpoint.\n",
            "\n",
            "--- Evaluating Models on Test Set ---\n",
            "\n",
            "Evaluating Separation Model (SDR)...\n",
            "Evaluating separation model on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Separation Eval:   0%|          | 0/113 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe7b72c4b63e4b089948343c6dc1f309"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SDR Debug (Batch 0, Item 0) ---\n",
            "  true_wav_i | Shape: torch.Size([220500]), Min: -0.5025, Max: 0.6137, Mean: 0.0011, Std: 0.1557\n",
            "  est_wav_i  | Shape: torch.Size([220500]), Min: -0.5030, Max: 0.5754, Mean: 0.0003, Std: 0.0632\n",
            "  Individual SDR calculated: 12.1899\n",
            "--- End SDR Debug ---\n",
            "\n",
            "--- SDR Debug (Batch 0, Item 1) ---\n",
            "  true_wav_i | Shape: torch.Size([220500]), Min: -0.7108, Max: 0.7418, Mean: 0.0001, Std: 0.1634\n",
            "  est_wav_i  | Shape: torch.Size([220500]), Min: -0.6768, Max: 0.5292, Mean: 0.0000, Std: 0.0939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c5505a587935>:52: FutureWarning: mir_eval.separation.bss_eval_sources\n",
            "\tDeprecated as of mir_eval version 0.8.\n",
            "\tIt will be removed in mir_eval version 0.9.\n",
            "  sdr_value, _, _, _ = mir_eval.separation.bss_eval_sources(true[np.newaxis, :], est[np.newaxis, :])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Individual SDR calculated: 2.0751\n",
            "--- End SDR Debug ---\n",
            "\n",
            "--- Average SDR on Test Set (from 3600 valid scores): 4.8087 ---\n",
            "\n",
            "Evaluating Classification Model (Baseline)...\n",
            "Evaluating classification model on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Classification Eval:   0%|          | 0/38 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1926cec1342c46aaaa04b75cbff3958e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.73      0.61       150\n",
            "           1       0.47      0.45      0.46       150\n",
            "           2       0.76      0.51      0.61       150\n",
            "           3       0.84      0.61      0.71       150\n",
            "           4       0.54      0.68      0.60       150\n",
            "           5       0.72      0.65      0.68       150\n",
            "           6       0.38      0.32      0.35       150\n",
            "           7       0.55      0.68      0.61       150\n",
            "\n",
            "    accuracy                           0.58      1200\n",
            "   macro avg       0.60      0.58      0.58      1200\n",
            "weighted avg       0.60      0.58      0.58      1200\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "Labels: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
            "[[109   9   0   4  12   5   7   4]\n",
            " [ 16  68   5   0  21   7  16  17]\n",
            " [  3  17  76   0  17   9  17  11]\n",
            " [ 35   4   0  92   2   5  11   1]\n",
            " [ 14  14   4   2 102   0   3  11]\n",
            " [ 16   7   2   3   9  97  13   3]\n",
            " [ 10  14   6   8  20   8  48  36]\n",
            " [  6  13   7   1   5   3  13 102]]\n",
            "--- Overall Accuracy: 0.5783 ---\n",
            "--- Weighted F1-Score: 0.5779 ---\n",
            "\n",
            "Evaluating Combined Model (Sequential)...\n",
            "Processing 1200 files for combined evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Combined Eval:   0%|          | 0/1200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ba61694983c4aa2af0025cf7c9644fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Combined (Sequential) Classification Report ---\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Electronic       0.55      0.67      0.60       150\n",
            " Experimental       0.48      0.47      0.47       150\n",
            "         Folk       0.72      0.53      0.61       150\n",
            "      Hip-Hop       0.82      0.65      0.72       150\n",
            " Instrumental       0.51      0.68      0.58       150\n",
            "International       0.73      0.60      0.66       150\n",
            "          Pop       0.34      0.31      0.32       150\n",
            "         Rock       0.56      0.67      0.61       150\n",
            "\n",
            "     accuracy                           0.57      1200\n",
            "    macro avg       0.59      0.57      0.57      1200\n",
            " weighted avg       0.59      0.57      0.57      1200\n",
            "\n",
            "--- Combined Accuracy: 0.5725 ---\n",
            "--- Combined Weighted F1-Score: 0.5735 ---\n",
            "\n",
            "Comparison: Baseline Acc=0.5783, F1=0.5779 | Combined Acc=0.5725, F1=0.5735\n",
            "\n",
            "--- Processing Case Study File ---\n",
            "Case study file not found or not defined: /content/drive/MyDrive/cw1_DL/Case_study_city.mp3\n",
            "\n",
            "=============================================\n",
            "=== Project Execution Finished ===\n",
            "Total execution time: 10.46 minutes\n",
            "=============================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "collapsed_sections": [
        "UBEG_juZQnzy",
        "iD0WT0NbZxN1",
        "PNYmRvaMX3xa",
        "5WaiurStYUWt",
        "UsxIHe9yZxN2",
        "kB-8C11RHeKt",
        "0Z1XNLEIHcFk",
        "1NYNsaYeHZlA",
        "XBYW5WZ3NCaE",
        "0568heFDwTY7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1d18f23223846c79cb8a062fc458241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b0e49ab9587427187c5a6872c9d1279",
              "IPY_MODEL_2f53fab508b54fdfbead493a19d29c59",
              "IPY_MODEL_d272fed9f32f4694b6f7aadd01219d3f"
            ],
            "layout": "IPY_MODEL_7757682b98db449da1fd8e6565c84571"
          }
        },
        "1b0e49ab9587427187c5a6872c9d1279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66aa94668b124984a914b720912fb5b0",
            "placeholder": "",
            "style": "IPY_MODEL_75e8923ece3b4866a79ddeb3df8a02c4",
            "value": "Epoch1/30[SeparationTrain]:2%"
          }
        },
        "2f53fab508b54fdfbead493a19d29c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_476d8e52c12045bc9470323ed98f9b05",
            "max": 525,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b02b3b25c0a24c75b08832d3858604f6",
            "value": 9
          }
        },
        "d272fed9f32f4694b6f7aadd01219d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01dcd83e6624e32b8c92472b7d690a1",
            "placeholder": "",
            "style": "IPY_MODEL_88c17211e6bf42fdb362fb97e01c987f",
            "value": "9/525[01:02&lt;49:40,5.78s/it,loss=0.4142]"
          }
        },
        "7757682b98db449da1fd8e6565c84571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66aa94668b124984a914b720912fb5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e8923ece3b4866a79ddeb3df8a02c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "476d8e52c12045bc9470323ed98f9b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02b3b25c0a24c75b08832d3858604f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f01dcd83e6624e32b8c92472b7d690a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c17211e6bf42fdb362fb97e01c987f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe7b72c4b63e4b089948343c6dc1f309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76a21e67fd864f2497263363a093ccd0",
              "IPY_MODEL_e3fb3ddeb1054e068b26d9b3c10cd321",
              "IPY_MODEL_9c5b96c29e1140a6a57bbda2a6e8afe3"
            ],
            "layout": "IPY_MODEL_c4b6aa7e5eaa4a3c99c714d27b7d3ea8"
          }
        },
        "76a21e67fd864f2497263363a093ccd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb459b8899664e7398f26ac58b51584a",
            "placeholder": "",
            "style": "IPY_MODEL_4ea5382b985c4917abe63d36f098a2c8",
            "value": "SeparationEval(AvgSDR:4.81):100%"
          }
        },
        "e3fb3ddeb1054e068b26d9b3c10cd321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a7af744ec3647d0b3f9408d658a4929",
            "max": 113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a929299b37114d85835938f0690a50ad",
            "value": 113
          }
        },
        "9c5b96c29e1140a6a57bbda2a6e8afe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2249e8c71d94742ad003492f272c56e",
            "placeholder": "",
            "style": "IPY_MODEL_03438b5f21d0438899491fe6f97570a2",
            "value": "113/113[06:47&lt;00:00,3.00s/it]"
          }
        },
        "c4b6aa7e5eaa4a3c99c714d27b7d3ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "bb459b8899664e7398f26ac58b51584a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ea5382b985c4917abe63d36f098a2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a7af744ec3647d0b3f9408d658a4929": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a929299b37114d85835938f0690a50ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2249e8c71d94742ad003492f272c56e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03438b5f21d0438899491fe6f97570a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1926cec1342c46aaaa04b75cbff3958e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d91a1cb5843469c88b488fc38792722",
              "IPY_MODEL_44f30a5bfa5c4160a746be53cc003bd2",
              "IPY_MODEL_08cfabf4657f42199b274dfe6cd592ba"
            ],
            "layout": "IPY_MODEL_a915e905c85440ddb642d183ed2bf2b2"
          }
        },
        "5d91a1cb5843469c88b488fc38792722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e009ce0de184d6794bef4a32307735e",
            "placeholder": "",
            "style": "IPY_MODEL_2c3fdbabe0f04fd8a3729082f785bdc9",
            "value": "ClassificationEval:97%"
          }
        },
        "44f30a5bfa5c4160a746be53cc003bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b6d52417604e828f9692e70b508b14",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9827f8e0bea246d0945bd911581ae199",
            "value": 38
          }
        },
        "08cfabf4657f42199b274dfe6cd592ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4beb416fc52f4cef909ab2dcd393911e",
            "placeholder": "",
            "style": "IPY_MODEL_5744afdbf91545e1b25d910fc37e5f72",
            "value": "37/38[00:51&lt;00:01,1.34s/it]"
          }
        },
        "a915e905c85440ddb642d183ed2bf2b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3e009ce0de184d6794bef4a32307735e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3fdbabe0f04fd8a3729082f785bdc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00b6d52417604e828f9692e70b508b14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9827f8e0bea246d0945bd911581ae199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4beb416fc52f4cef909ab2dcd393911e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5744afdbf91545e1b25d910fc37e5f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ba61694983c4aa2af0025cf7c9644fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56068fa5fede49a1b9f7512589b94b6f",
              "IPY_MODEL_423dbb8ca1fa4043800263529ece78a6",
              "IPY_MODEL_2e86a8bf19944067909cbc31246b9f65"
            ],
            "layout": "IPY_MODEL_7ac87150e4944ede83a989e5cd99857e"
          }
        },
        "56068fa5fede49a1b9f7512589b94b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95782dc31c34acbb1462d8002f329cc",
            "placeholder": "",
            "style": "IPY_MODEL_55303a91b04a44af99935c8a9803a924",
            "value": "CombinedEval:100%"
          }
        },
        "423dbb8ca1fa4043800263529ece78a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed5e375ee0a4174ab25407f48dfe0e7",
            "max": 1200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbb110b3e02948d6b1c17a3ba2f9bc1b",
            "value": 1200
          }
        },
        "2e86a8bf19944067909cbc31246b9f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_611a4604ceba4e8787583b6131ee214c",
            "placeholder": "",
            "style": "IPY_MODEL_442f572003b3466893b0e6c982993d17",
            "value": "1200/1200[02:47&lt;00:00,7.02it/s]"
          }
        },
        "7ac87150e4944ede83a989e5cd99857e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b95782dc31c34acbb1462d8002f329cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55303a91b04a44af99935c8a9803a924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed5e375ee0a4174ab25407f48dfe0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb110b3e02948d6b1c17a3ba2f9bc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "611a4604ceba4e8787583b6131ee214c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442f572003b3466893b0e6c982993d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}